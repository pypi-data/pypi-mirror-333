Metadata-Version: 2.1
Name: gguf-comfy
Version: 0.2.1
Summary: run flux1/sd3 model with beginner GPU (low cost) or even CPU
Author-email: calcuis <info@calcu.io>
Description-Content-Type: text/markdown
Classifier: License :: OSI Approved :: MIT License
Requires-Dist: tqdm
Project-URL: Download, https://github.com/calcuis/gguf/releases
Project-URL: GitHub, https://github.com/calcuis/gguf-comfy
Project-URL: Homepage, https://gguf.org
Project-URL: reddit, https://www.reddit.com/r/gguf

### gguf-comfy

node diagram generator (image/audio/video)

#### enter the menu by
```
py -m gguf_comfy
```
>Please select:
>1. download gguf pack
>2. download comfy pack
>
>Enter your choice (1 to 2): _

#### gguf pack (option 1)
generic pack with gguf as core node

#### comfy pack (option 2)
comfy pack with gguf as custom node

![screenshot](https://raw.githubusercontent.com/calcuis/gguf-comfy/master/gguf.png)

- decompress the 7z bundle file: Extract All... (it includes everything you need to run a model)
- you could either get the dry run pack [here](https://huggingface.co/calcuis/flux-dev-gguf/tree/main) or pick them one-by-one from the source below
- download [flux-dev-fp32-q4_0.gguf](https://huggingface.co/calcuis/flux-dev-gguf/blob/main/flux-dev-fp32-q4_0.gguf) (6.71GB); pull it to ./ComfyUI/models/diffusion_models
- download [clip_l_fp32-f16.gguf](https://huggingface.co/calcuis/flux-dev-gguf/blob/main/clip_l_fp32-f16.gguf) (246MB) and [t5xxl_fp32-q4_0.gguf](https://huggingface.co/calcuis/flux-dev-gguf/blob/main/t5xxl_fp32-q4_0.gguf) (2.75GB); pull them to ./ComfyUI/models/text_encoders
- download [pig_flux_vae_fp32-f16.gguf)](https://huggingface.co/calcuis/flux-dev-gguf/blob/main/pig_flux_vae_fp32-f16.gguf) (168MB); pull it to ./ComfyUI/models/vae
- run the .bat file under the main directory (it will activate the py backend as well as the js frontend)
- drag [workflow-flux-dev.json](https://huggingface.co/calcuis/flux-dev-gguf/blob/main/workflow-flux-dev.json) to the activated browser

You are good to GO! (now you can run flux1 with the cheapest Nvidia GPU or merely CPU) ENJOY!
#### Reference: [flux](https://github.com/black-forest-labs/flux) [city96](https://github.com/city96/ComfyUI-GGUF) [comfyanonymous](https://github.com/comfyanonymous/ComfyUI) [gguf](https://github.com/calcuis/gguf)

![screenshot](https://raw.githubusercontent.com/calcuis/comfy/master/sd3.png)
#### Stable Diffusion 3.5 is now supported (please upgrade to the latest version)

Run sd3.x model following the step-by-step guide below:
- you could either get the backup pack [here](https://huggingface.co/calcuis/sd3.5-large-gguf/tree/main) or pick them one-by-one from the original source below
- download [sd3.5_large-q4_0.gguf](https://huggingface.co/calcuis/sd3.5-large-gguf/blob/main/sd3.5_large-q4_0.gguf) (4.77GB); pull it to ./ComfyUI/models/diffusion_models
- download [clip_g.safetensors](https://huggingface.co/stabilityai/stable-diffusion-3.5-large/blob/main/text_encoders/clip_g.safetensors) (1.39GB),  [clip_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/blob/main/clip_l.safetensors) (234MB), and [t5xxl_fp8_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/blob/main/t5xxl_fp8_e4m3fn.safetensors) (4.55GB); pull them all to ./ComfyUI/models/text_encoders
- download [diffusion_pytorch_model.safetensors](https://huggingface.co/stabilityai/stable-diffusion-3.5-large/blob/main/vae/diffusion_pytorch_model.safetensors) (168MB); pull it to ./ComfyUI/models/vae
- run the .bat file under the main directory (it will activate the py backend as well as the js frontend)
- drag [workflow-gguf-sd3.5.json](https://huggingface.co/calcuis/gguf-node/blob/main/workflow-gguf-sd3.5.json) to the activated browser

#### Reference: [stability-ai](https://github.com/Stability-AI/sd3.5) [gguf](https://github.com/calcuis/gguf)
