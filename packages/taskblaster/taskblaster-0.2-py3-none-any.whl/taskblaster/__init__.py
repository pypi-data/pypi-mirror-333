from __future__ import annotations

import functools
import os
import shutil
import traceback
from abc import ABC, abstractmethod
from dataclasses import dataclass
from inspect import isfunction
from pathlib import Path
from typing import Any, Dict, Iterable

from taskblaster.abc import ErrorHandler
from taskblaster.state import State


__version__ = '0.2'


class WardenPanic(Exception):
    """When the Warden does something it shouldn't we panic"""


class TBUserError(Exception):
    """Errors in workflows or other components controlled by user.

    The design goal of TaskBlaster is that any error which is generated by
    user, for example invalid workflow, should only raise TBUserError
    inside the TaskBlaster source code. Otherwise, it might be inconclusive
    whether it is internal error or error cause by the users actions."""


class TBReadOnlyRepository(Exception):
    """Attempt to modify a read only repository."""


# Name for the default branch (i.e. branch name used for branches without
# @tb.branch decorator)
ENTRY_BRANCH = 'entry'

# Database name for unreachable reference
# Instance of UnreachableReference will be stored to db with this name
UNREACHABLE_REF = '__tb_unreachable__'

# Magical name of workflow initializer task:
INITNAME = 'init'


def writes(meth):
    """Decorator to throw a proper exception upon write attempt"""

    @functools.wraps(meth)
    def wrapper(self, *args, **kwargs):
        if self.read_only:
            from taskblaster import TBReadOnlyRepository

            raise TBReadOnlyRepository
        return meth(self, *args, **kwargs)

    return wrapper


class Accessor:
    def _accessor(self, item, index):
        raise NotImplementedError

    def __getitem__(self, index):
        """Reference of index into output of this task-like object.

        Return new reference which is an index into the return value of
        another task.

        For example a workflow can do tb.node(x=self.othertask['hello']).

        """
        return self._accessor(GETITEM, index)

    def __getattr__(self, name):
        """Reference via attributes into output of this task like object.

        Return new reference which is an index into the return value of
        another task.

        For example a workflow can do tb.node(x=self.othertask.hello).

        """
        assert name != '__task__'

        return self._accessor(GETATTR, name)

    def __call__(self, *args, **kwargs):
        """Reference a call to a task like object.

        Return new reference which is an index into the return value of
        another task.

        For example a workflow can do
            tb.node(x=self.othertask.hello(a=self.thirdtask)),
        if the othertask would return an object, which has a method
        called hello.
        """
        return self._accessor(FUNCCALL, (args, kwargs))

    @classmethod
    def accessor_index(cls, value, index, resolve_reference=False):
        for _type_and_subindex in index:
            if resolve_reference and isinstance(
                value, BoundWorkflowSpecification
            ):
                value = value.get_node()
            if isinstance(_type_and_subindex, (list, tuple)):
                _type, subindex = _type_and_subindex
                if _type == GETITEM:
                    try:
                        value = value[subindex]
                    except Exception as e:
                        raise TBUserError(
                            f'Cannot index {value} with {subindex}: {e}.'
                        ) from None
                elif _type == GETATTR:
                    try:
                        value = getattr(value, subindex)
                    except Exception as e:
                        raise TBUserError(
                            f'{value} does not have an attribute '
                            f'{subindex}: {e}.'
                        ) from None
                elif _type == FUNCCALL:
                    try:
                        value = value(*subindex[0], **subindex[1])
                    except Exception as e:
                        raise TBUserError(
                            'Following exception occurred when calling a'
                            f'function inside the workflow definition: {e}.'
                        )
                else:
                    assert 0

            else:
                try:
                    value = value[_type_and_subindex]
                except Exception as e:
                    raise TBUserError(
                        f'Cannot index {value} with {_type_and_subindex}: {e}.'
                    ) from None

        return value


def record(item):
    assert isinstance(item, Accessor)
    return item._accessor(RECORD, 'record')


@dataclass
class Reference:
    """Simplest possible class to represent a dependency.

    It consisists of full name of the task and possible indexing of the
    tasks output. BoundTaskSpecification is a superclass, but internally
    the class is used by taskblaster to create relationships between
    tasks.
    """

    name: str
    index: tuple = tuple()

    def _tb_pack(self):
        """_tb_pack attribute is to be set when packing kwargs of a task
        specification, and it will return a json-serializable dictionary.
        """
        return {'__tb_type__': 'ref', 'name': self.name, 'index': self.index}

    @property
    def unreachable(self):
        return False

    def resolve_reference(self):
        return self


class UnreachableReference(Reference):
    """Reference which cannot ever be met.

    This is used for placeholder for a task which is not supposed to have its
    dependencies met, but we do not know its dependencies yet either.
    """

    def __init__(self):
        super().__init__(UNREACHABLE_REF)

    @property
    def unreachable(self):
        return True


class HashedExternalFile:
    def __init__(self, path, digest):
        self.path = path
        self.digest = digest

    def __repr__(self):
        return f'HashedExternalFile({self.path} [{self.digest}])'

    def tb_encode(self):
        return dict(path=str(self.path), digest=self.digest)

    @classmethod
    def tb_decode(cls, data):
        return cls(**data)


class ExternalFile:
    def __init__(self, path):
        from pathlib import Path

        # XXX should be resolved relative to root
        self.path = Path(path).resolve()

    def tb_encode(self):
        return str(self.path)

    @classmethod
    def tb_decode(cls, data):
        return cls(data)

    def _filehash(self):
        from hashlib import sha256

        chunksize = 2**16
        hashobj = sha256()
        with self.path.open('rb') as fd:
            while True:
                chunk = fd.read(chunksize)
                if not chunk:
                    break
                hashobj.update(chunk)
        return hashobj.hexdigest()

    def hashed(self):
        digest = self._filehash()
        return HashedExternalFile(self.path, digest)

    def __repr__(self):
        return f'ExternalFile({self.path!r})'


class InputVariable:
    """Declaration of a workflow input variable.

    Use the function :func:`var` to get an instance of
    :class:`InputVariable`. When assigned as a class attribute to the workflow
    class, it signifies that the workflow will have a parameter,
    with name of the attribute it is assigned to.

    ::

        @tb.workflow
        class MyWorkflow:
            # Following two declarations are equivalent
            my_input_parameter = tb.var(default='mydefault')
            my_input_parameter2 = tb.InputParameter(default='mydefault')
    """

    def __init__(self, default):
        self._default = default


_no_default = object()


def var(default=_no_default) -> InputVariable:
    """Define an input variable for a workflow.

    If ``default`` is specified, the input variable becomes optional.

    Example::

      @tb.workflow
      class MyWorklow:
          x = tb.var()
          y = tb.var(default=42)

      wf = MyWorkflow(x=17)
    """
    return InputVariable(default)


class TBProperty(property):
    """Superclass for all workflow properties

    Currently, this includes
      * task
      * subworkflow
      * dynamical_workflow_genarator
    decorators.

    XXX To be extended further.
    """

    def __init__(self, *, fget):
        super().__init__(fget=fget)
        self.branch = ENTRY_BRANCH
        self.jump = None
        self._if = None
        self.external = False
        self.loop = False
        self.has_record = False
        self.error_handlers = None


class TaskBlasterInterrupt(BaseException):
    pass


class TaskView:
    """View of a task object for use with actions.

    Actions associated with a task will receive this object as input."""

    def __init__(self, indexnode, future):
        self._future = future
        self.node = indexnode
        # XXX deprecate this name (asrlib uses it)

    @property
    def input(self):
        """Dictionary of task inputs with references."""
        # We return an empty placeholder string for the historical digest:
        return '', self._future._entry.read_inputfile()

    @property
    def output(self):
        """Output of task, i.e., the return value of the task function.

        If task is not done, the output will be None."""
        return (
            self._future._entry.output()
            if self._future._entry.has_output()
            else None
        )

    @property
    def directory(self):
        """The directory of the task.

        Use this to access files generated by the task."""
        return self._future.directory

    @property
    def realized_input(self):
        """Dictionary of task inputs with references resolved to objects."""
        return self._future._actual_inputs


def actions(**actions):
    """Return decorator for task function to provide custom actions."""

    def deco(func):
        func.__tb_actions__ = actions
        return func

    return deco


# XXX To be removed, by flagging the task to by a dynamical workflow
# task by some other way
def dynamical_workflow_generator_task(f):
    f._tb_dynamical_workflow_generator_task = True
    return f


def dynamical_workflow_generator(result_tasks=None):
    """Decorator to specify dynamical subworkflow generator.

    Takes a keyword argument called result_tasks.
    """
    return DynamicalWorkflowGeneratorSpecificationProperty.decorator(
        result_tasks=result_tasks
    )


def create_record_for_task(
    taskname: str,
    parent_state,
    cache=None,
    output_as: str = 'Task',
):
    from taskblaster.namedtask import Task

    parent = parent_state == State.fail or [(taskname, Reference(taskname))]
    node = Node(
        'taskblaster.records.create_task_record',
        {'name': taskname, '__tb_record__': parent},
    )

    if output_as == 'TaskSpec':
        return node

    namedtask = Task(f'{taskname}.record', node, branch='entry', source='')
    if output_as == 'Task':
        return namedtask

    if output_as == 'EncodedTask':
        return cache.json_protocol.encode_task(namedtask)

    assert False


class BoundReference(Reference, Accessor):
    def __init__(self, name, declaration, workflow, index=tuple()):
        assert index is not None
        super().__init__(name, index)
        self.declaration = declaration
        self.workflow = workflow

    def previous_loop(self):
        name, iteration = self.name.rsplit('-', 1)
        name = f'{name}-{int(iteration) - 1:03d}'
        return self.__class__(
            name, self.declaration, self.workflow, index=self.index
        )

    def resolve_reference(self):
        return self

    @property
    def jump(self):
        return self.declaration.jump

    @property
    def _if(self):
        return self.declaration._if

    @property
    def external(self):
        return self.declaration.external

    @property
    def branch(self):
        return self.declaration.branch

    @property
    def has_record(self):
        return self.declaration.has_record

    @property
    def error_handlers(self):
        return self.declaration.error_handlers

    def __repr__(self):
        return (
            f'{type(self).__name__}({self.name!r}, {self.declaration}, '
            f'{self.workflow}, {self.index})'
        )

    def _accessor(self, code, index):
        newindex = (*self.index, (code, index))
        if code == RECORD:

            def taskrecord(tasks_self):
                return create_record_for_task(
                    self.name,
                    parent_state=State.done,
                    output_as='TaskSpec',
                )

            prop = TaskSpecificationProperty(taskrecord)
            return self.__class__(
                f'{self.name}.record', prop, self.workflow, self.index
            )
        return self.__class__(
            self.name, self.declaration, self.workflow, newindex
        )


class BoundTaskSpecification(BoundReference):
    def get_node(self):
        """At workflow level, call wrapped unbound method to produce node."""
        try:
            return self.declaration.unbound_meth(self.workflow)
        except Exception as ex:
            raise UserWorkflowError from ex

    def __task__(self):
        from taskblaster.namedtask import Task

        node: Node = self.get_node()

        source = self.workflow._rn.get_full_name('.')
        if source == '.':
            source = ''

        kwargs = {}
        if self.external:
            kwargs['__tb_external__'] = True

        return Task(
            self.name,
            node.replace(**kwargs),
            branch=self.branch,
            source=source,
            tags=self.declaration.tags or set(),
            has_record=self.has_record,
            error_handlers=self.error_handlers,
            jump=self.jump,
            _if=self._if,
        )


class BoundWorkflowSpecification(BoundReference):
    def _tb_pack(self):
        """_tb_pack attribute is to be set when packing kwargs of a task
        specification, and it will return a json-serializable dictionary.
        """

        return self.resolve_refence()._tb_pack()

    def resolve_reference(self):
        obj = self.accessor_index(self, self.index, resolve_reference=True)
        return obj

    def get_node(self):
        """At workflow level, call wrapped unbound method to produce node."""
        try:
            subworkflow = self.declaration.unbound_meth(self.workflow)
            subworkflow._rn = self.workflow._rn.with_directory(self.name)
            return subworkflow
        except Exception as ex:
            raise UserWorkflowError from ex


class BoundDynamicalWorkflowGeneratorSpecification(BoundWorkflowSpecification):
    def get_node(self):
        try:
            init_node = self.declaration.unbound_meth(self.workflow)
        except Exception as ex:
            raise UserWorkflowError from ex

        result_tasks = self.declaration.result_tasks
        init_node.kwargs.update(__tb_result_tasks__=result_tasks)

        class DynamicalWorkflow:
            __tb_dynamical_init__ = True

            @task
            def init(self):
                return init_node

        def get_result_task_predeclaration(name):
            def fun(self):
                return node('define', obj=[UnreachableReference(), self.init])

            fun.__name__ = name
            fun.__qualname__ = name
            return fun

        for taskname, glob_pattern in result_tasks.items():
            setattr(
                DynamicalWorkflow,
                taskname,
                task(get_result_task_predeclaration(taskname)),
            )

        # Decorate it after-the-fact (so that one picks up the setattr)
        DynamicalWorkflow = workflow(DynamicalWorkflow)

        dynamicalworkflow = DynamicalWorkflow()
        dynamicalworkflow._rn = self.workflow._rn.with_directory(self.name)
        return dynamicalworkflow


class WorkflowMethod(TBProperty):
    bound_class: type | None = None

    def __init__(
        self,
        unbound_meth,
        *,
        tags=None,
        has_record=False,
        error_handlers=None,
    ):
        super().__init__(fget=self._fget)
        self.unbound_meth = unbound_meth
        self.tags = tags
        self.has_record = has_record
        if error_handlers is None:
            error_handlers = []
        elif isinstance(error_handlers, Iterable):
            error_handlers = [*error_handlers]
        elif isinstance(error_handlers, ErrorHandler):
            error_handlers = [error_handlers]
        else:
            raise TypeError(
                f'Cannot understand error handler {error_handlers}'
            )

        self.error_handlers = error_handlers

    @property
    def methname(self):
        return self.unbound_meth.__name__

    def _fget(self, workflow):
        name = workflow._rn.get_full_name(self.methname)
        if self.loop:
            name += '-%03d' % workflow._rn.state._seen_branches[self.branch]

        return self.bound_class(name, self, workflow)

    @classmethod
    def decorator(cls, **kwargs):
        return lambda method: cls(method, **kwargs)

    def __repr__(self):
        # XXX Display special attributes only if they are set
        return (
            f'WorkflowMethod(bound_class={self.bound_class.__name__},'
            f' unbound_meth={self.unbound_meth.__name__!r}, '
            f'branch={self.branch!r}, '
            f'_if={self._if}, loop={self.loop}, '
            f'jump={self.jump}, branch={self.branch})'
        )


class WorkflowSpecificationProperty(WorkflowMethod):
    bound_class = BoundWorkflowSpecification


class TaskSpecificationProperty(WorkflowMethod):
    bound_class = BoundTaskSpecification


class DynamicalWorkflowGeneratorSpecificationProperty(WorkflowMethod):
    bound_class = BoundDynamicalWorkflowGeneratorSpecification
    _tb_dynamical_workflow_generator = True

    def __init__(self, *args, result_tasks=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.result_tasks = result_tasks


class UserWorkflowError(Exception):
    pass


GETITEM = '['
GETATTR = '.'
PARENT = 'P'
RECORD = 'R'
SUBFOLDER = '/'
FUNCCALL = '('


def fixedpoint(_property):
    """Specifies that this is an OUTPUT of a workflow, to which other
    workflows can depend on.

    The fixedpoint decorator is required for a subworkflow, which is
    dynamic, and the master workflow wants to depend on a task,
    which is not in the entry branch.

    Utilizing the fixed point decorator, will create a dummy placeholder
    for the master workflow to depend on, until the control flow of the
    subworkflow reaches this output `fixed point`::

        @tb.workflow
        class DynamicSubWorkflow:
            ...

            @tb.branch('loop')
            @tb.task
            def relax(self):
                return tb.node('relax', input=self.Phi(...))

            ...


            @tb.branch('final')
            @tb.fixedpoint # <-- this must be defined as fixed point
            @tb.task
            def converged_result(self):
                return tb.node('postprocess', input=tb.relax)

        @tb.workflow
        class MyWorkflow:
            @tb.subworkflow
            def dynamic_subworkflow(self):
                return DynamicSubworkflow()

            @tb.task # <-- for this task to depend on non-entry branch task
            def depend_on_dynamic_result(self):
                return tb.node('post_process',
                               inp=self.dynamic_subworkflow.converged_result)

    The reason for required to predeclare fixedpoint is that at `MyWorkflow`,
    both the subworkflow and the `depend_on_dynamic_result` are on the same
    branch, and hence made into tasks simultaneously (with topological sort).
    If the converged_result tasks of the subworkflow wouln't exist yet,
    `depend_on_dynamic_result` wouldn't be able to depend on it.

    """
    if not isinstance(_property, TaskSpecificationProperty):
        raise TBUserError(
            '@taskblaster.fixedpoint decorator should be above'
            ' @taskblaster.task'
        )
    _property.external = True
    return _property


def jump(branch_name):
    """Specifies that there is an jump to another branch from this branch."""

    def wrapper(_property):
        if not isinstance(_property, TBProperty):
            raise TBUserError(
                '@taskblaster.jump decorator should be above'
                ' @taskblaster.task/subworkflow decorators'
            )
        _property.jump = branch_name
        return _property

    return wrapper


def _if(dct=None, *, true=None, false=None):
    """Specifies that there is a CONDITIONAL jump to another branch.

    If the result returned by the task decorated evaluated to True,
    true branch is selected, and otherwise false branch is selected.
    """

    def wrapper(_property):
        if not isinstance(_property, TaskSpecificationProperty):
            raise TBUserError(
                '@taskblaster._if decorator should be above @taskblaster.task'
            )

        if dct is not None:
            if true is not None or false is not None:
                raise TBUserError(
                    '@taskblaster._if decorator should be used either '
                    'by passing a switch dictionary or true, false keywords, '
                    'not both.'
                )
            _property._if = dct
        else:
            _property._if = {True: true, False: false}

        return _property

    return wrapper


class Phi(Accessor):
    """Operator to determine an input to a task depending on the from branch.

    For example, if there is a divergence of branches, Phi operator can be
    utilized to converge and gather output from multiple tasks. Essentially
    replaced the use of variables in context of workflows.

    As a practical example, consider a while loop, which would perform some
    kind of relaxation to an object until converged. Naturally, the first time
    the relaxation loop is entered, the input object must come from actual
    input of the workflow, but on the subsequent steps, the input object must
    come from the result of the previous step. This would be decribed as::

        @tb.branch('relaxbranch')
        @tb.task
        def relax(self):
            return tb.node('postprocess', object=tb.phi(relaxbranch=self.relax,
                                                        entry=self.initial_object)

    Note that the name of the default starting branch is always `entry`.

    Takes as an input a dictionary, where the keys determine the branch names
    one is jumping from. A complete example
    (see also the :ref:`tutorial_dynamical` tutorial and
    :ref:`explanation_dynamical` explanation)::

        @tb.workflow
        class Workflow:
            @tb.branch('entry')
            @tb._if(true='truebranch', false='falsebranch')
            @tb.task
            def evaluate(self):
                return tb.node('decide_branch', input=self.inputA)

            @tb.branch('truebranch')
            @tb.jump('finalbranch')
            @tb.task
            def calc_true(self):
                return tb.node('calculateA', input=self.inputA)

            @tb.branch('falsebranch')
            @tb.jump('finalbranch')
            @tb.task
            def calc_false(self):
                return tb.node('calculateB', input=self.inputA)

            @tb.branch('finalbranch')
            @tb.task
            def final_result(self):
                return tb.node('postprocess',
                               output=self.Phi(truebranch=self.calc_true,
                                             falsebranch=self.calc_false))

    In the following code we indicate, that postprocess task will take an input
    argument named `output` and it will either take it from calculate_true or
    calculate_false branches, depending to which branch the control flow came
    from to the `finalbranch`. This pattern would be equivalent to following
    pseudo code::

        if decide_branch(inputA):
            tmp = calculateA(input=inputA)
        else:
            tmp = calculateB(input=inputA)
        final_result = postprocess(output=tmp)

    """

    UNRESOLVED = '<unresolved Phi operator>'

    def __init__(
        self,
        index=tuple(),
        _rn=None,
        resolved=UNRESOLVED,
        debug=False,
        **kwargs,
    ):
        if _rn is None:
            import warnings

            warnings.warn(
                'tb.Phi(...) will be removed. Use self.Phi(...) instead.',
                FutureWarning,
            )
        self.index = index
        self.kwargs = kwargs
        self.resolved = resolved
        self.debug = debug
        self._rn = _rn
        self.creator_stack = traceback.extract_stack(limit=8)

    def __repr__(self):
        info = 'RESOLVED' if self.resolved != Phi.UNRESOLVED else 'UNRESOLVED'
        return f'Phi({id(self)}, {info}, {self.kwargs})'

    def resolve(self, default_runner):
        def debug(x):
            if self.debug:
                print('Phi:', x)

        runner = self._rn or default_runner
        debug(f'resolving... {self}')
        debug(f'runner: {runner._directory}')
        debug(f'_from_branch: {runner.state._from_branch}')

        def get_stack():
            # We will drop out two top elements of the stack,
            # to keep the bottom line to be users code.
            # So make sure we drop. If we are confused,
            #  we display the full stack.
            omit = -2
            for index in [-1, -2]:
                fname = self.creator_stack[index].filename
                if not fname.endswith('taskblaster/__init__.py'):
                    omit = None
            return ''.join(traceback.format_list(self.creator_stack[:omit]))

        if not (set(self.kwargs) & set(runner.state._seen_branches)):
            raise ValueError(
                'Error resolving a Phi-operator. '
                f'No branches in Phi ({set(self.kwargs)}) '
                'have yet been visited. Check that the task '
                'using Phi has correct branch decorator.\n'
                'Visited branches: \n'
                f'{set(runner.state._seen_branches)}\n' + get_stack()
            )

        try:
            resolved = self.kwargs[runner.state._from_branch]
        except KeyError:
            raise ValueError(
                'Phi operator not defined for from_branch '
                f'{runner.state._from_branch}. Have branches '
                f'{self.kwargs.keys()} on workflow {runner.directory}.\n'
                f'Stack trace of offending Phi operator:\n{get_stack()}'
            )

        debug(f'resolved from kwargs {resolved}')

        if isfunction(resolved):
            resolved = resolved()

        if (
            resolved is not None
            and runner.state._from_branch == runner.state._current_branch
        ):
            if isinstance(resolved, BoundReference):
                assert resolved.branch == runner.state._from_branch
                debug('Calling previous loop.')
                resolved = resolved.previous_loop()
                debug(f'Previous loop resolved to {resolved}.')

        debug(f'resolved before accessors {resolved}')
        # assert self.resolved is Phi.UNRESOLVED
        for _type, name in self.index:
            resolved = resolved._accessor(_type, name)

        debug(f'resolved after accessors {resolved}')
        self.resolved = resolved
        return self.resolved

    def _tb_pack(self):
        if self.resolved == Phi.UNRESOLVED:
            raise Exception(
                'Internal Taskblaster Error: Unresolved Phi operator'
                f'{self} {self.index} {self.kwargs}'
            )
        return self.resolved

    def _accessor(self, code, index):
        newindex = (*self.index, (code, index))
        return Phi(
            index=newindex, _rn=self._rn, resolved=self.resolved, **self.kwargs
        )


def branch(name, loop=False):
    """Specifies that a task or a subworkflow is to belong to a particular
    branch.
    """

    def wrapper(_property):
        if not isinstance(_property, TBProperty):
            if callable(_property):
                add = ' Missing @taskblaster.task?'
            else:
                add = ''
            raise UserWorkflowError(
                '@taskblaster.branch decorator not allowed'
                f'for {_property}.{add}'
            )
        _property.branch = name
        _property.loop = loop
        return _property

    return wrapper


def external_file(meth):
    @functools.wraps(meth)
    def task_fun(self):
        # Calls the method at Workflow class level to get the
        # BoundTaskSpecification reference to the file
        argument = meth(self)
        assert isinstance(argument, (Input, BoundTaskSpecification, str, Path))
        return Node(external_file_task, kwargs={'path': argument})

    return task(task_fun)


def task(_meth=None, **kwargs):
    """Decorator to specify tasks within the workflow class.

    Returns a property of type TaskSpecificationProperty.
    """
    deco = TaskSpecificationProperty.decorator(**kwargs)

    if _meth is None:
        return deco
    else:
        assert not kwargs
        return deco(_meth)


def subworkflow(_meth):
    """Decorator to specify subworkflows within the workflow class.

    Returns a property of type WorkflowSpeificationProperty.
    """
    deco = WorkflowSpecificationProperty.decorator()
    return deco(_meth)


class Node:
    def __init__(self, target, kwargs):
        self.target = target
        self.kwargs = kwargs

    def replace(self, target=None, **kwargs) -> Node:
        if target is None:
            target = self.target
        return Node(target, {**self.kwargs, **kwargs})

    def signature(self) -> str:
        tokens = ', '.join(
            f'{name}={value!r}' for name, value in self.kwargs.items()
        )
        return f'{self.target}({tokens})'

    def __repr__(self) -> str:
        return f'<node({self.target}, {self.kwargs})'


def node(target, **kwargs) -> Node:
    return Node(target, kwargs)


def define(meth):
    @functools.wraps(meth)
    def wrapper(self):
        obj = meth(self)
        return node('define', obj=obj)

    return task(wrapper)


class Input(Accessor):
    def __init__(self, value, *, _index=tuple()):
        # If the input variable has no default, it actually defaults to
        # the unique _no_default object which means we don't allow it.
        assert value is not _no_default
        self._value = value
        self.index = _index
        self.input_name = None

    def __repr__(self):
        indextext = ''.join(f'[{part!r}]' for part in self.index)
        return f'<Input({self._value!r}){indextext}>'

    def _tb_pack(self):
        return self.getvalue()

    def getvalue(self):
        return self.accessor_index(self._value, self.index)

    def _accessor(self, access_type, item):
        return Input(self._value, _index=(*self.index, (access_type, item)))


def totree(definitions, name, root=None):
    def workflow(rn):
        if root is not None:
            rn = rn.with_subdirectory(root)

        for key in definitions:
            obj = definitions[key]
            rn1 = rn.with_subdirectory(key)
            fullname = f'{key}/{name}'
            source = ''  # Who is the source?  It will be "root workflow"
            # the way we set it here.  Which might be okay.  After all
            # once we move fully to serialized workflows, totree()
            # will not be a toplevel workflow anymore so this case
            # disappears completely.
            rn1.define(obj, fullname, source=source)

    return workflow


class DummyUnboundTask(Accessor):
    """XXX To be removed and replaced with reference"""

    def __init__(self, future, *, _index=tuple()):
        self.future = future
        self.name = str(future.directory.relative_to(future._cache.directory))

        self.index = _index

    def _refhook(self):
        return self

    def _tb_pack(self):
        name = Path(self.name)
        indices = tuple()
        for operation, index in self.index:
            if operation == SUBFOLDER:
                name /= index
                # Subfolder only allowed in the beginning of index
                assert len(indices) == 0
            elif operation == PARENT:
                name = name.parent
                # Parent only allowed in the beginning of index
                assert len(indices) == 0
            else:
                indices = (*indices, (operation, index))

        return {
            '__tb_type__': 'ref',
            'name': str(name),
            'index': indices,
        }

    def _accessor(self, access_style, index):
        return DummyUnboundTask(
            self.future, _index=(*self.index, (access_style, index))
        )

    def __truediv__(self, index):
        return DummyUnboundTask(
            self.future, _index=(*self.index, (SUBFOLDER, index))
        )

    @property
    def parent(self):
        return DummyUnboundTask(
            self.future, _index=(*self.index, (PARENT, None))
        )

    @property
    def unreachable(self):
        return False


def glob_tasks(cache, pattern):
    for indexnode in cache.registry.index.glob_simple(pattern):
        future = cache[indexnode.name]
        path = future.directory
        relpath = path.relative_to(cache.directory)
        yield future, relpath


def parametrize_glob(pattern, *, globmethod=None):
    if globmethod is not None:
        import warnings

        warnings.warn(
            'Please do not set globmethod, which is now ignored.  '
            'This keyword will be removed.',
            FutureWarning,
        )

    def wrap(workflow_cls):
        def workflow(rn):
            cache = rn._cache

            assert rn.directory.samefile(cache.directory)

            # XXX This should be a kind of query instead of
            # looping over all the values.

            for future, relpath in glob_tasks(cache, pattern):
                unbound_task = DummyUnboundTask(future)

                actual_workflow = workflow_cls(unbound_task)
                rn1 = rn.with_subdirectory(relpath.parent)
                rn1.run_workflow(actual_workflow)

        return workflow

    return wrap


class BranchSpecification:
    def __init__(
        self, unbound_tasks, subworkflows, dynamical_workflow_generators
    ):
        self.unbound_tasks = unbound_tasks
        self.subworkflows = subworkflows
        self.dynamical_workflow_generators = dynamical_workflow_generators

        branching_tasks = []
        loop = None
        for unbound_task in unbound_tasks.values():
            if unbound_task.loop is not None:
                if loop is not None:
                    if unbound_task.loop != loop:
                        raise TBUserError(
                            'All branch definitions with equal name must'
                            ' have same loop=True/False condition.'
                        )
                loop = unbound_task.loop
            if unbound_task.jump is not None:
                branching_tasks.append(unbound_task)
            # Deliberately not elif here to catch _if and jump both defined
            # error!
            if unbound_task._if is not None:
                branching_tasks.append(unbound_task)

        self.loop = loop

        if len(branching_tasks) > 1:
            raise TBUserError(
                'Only one branching decorator per branch allowed.'
            )

        # Figure out which task branches,
        # and to which branches this task can branch to
        self.jumps = []
        self.if_task = None
        if len(branching_tasks) == 1:
            task = branching_tasks[0]
            if task.jump is not None:
                self.jumps = [task.jump]
            if task._if is not None:
                self.jumps = [
                    target for target in task._if if target is not None
                ]
                self.if_task = task

    @classmethod
    def from_workflow_class(cls, workflow_cls):
        branch_names = []
        for name, value in inherited_classvars(workflow_cls).items():
            if isinstance(value, TBProperty):
                branch_names.append(value.branch)

        # Make sure the is a dictionary corresponding to entry branch
        if ENTRY_BRANCH not in branch_names:
            branch_names.append(ENTRY_BRANCH)
            raise TBUserError(
                f'Workflow has no tasks in the {ENTRY_BRANCH} branch.'
            )

        # Add properties from all branches
        branches = {
            branch_name: cls.by_name(workflow_cls, branch_name)
            for branch_name in branch_names
        }
        return branches

    @classmethod
    def by_name(cls, workflow_cls, branch_name):
        unbound_tasks = {}
        subworkflows = {}
        dynamical_workflow_generators = {}

        def correct_branch(_property):
            return branch_name == _property.branch

        for name, value in inherited_classvars(workflow_cls).items():
            if isinstance(value, TaskSpecificationProperty):
                if not correct_branch(value):
                    continue
                unbound_tasks[name] = value
            elif isinstance(value, WorkflowSpecificationProperty):
                if not correct_branch(value):
                    continue
                subworkflows[name] = value
            elif hasattr(value, '_tb_dynamical_workflow_generator'):
                if not correct_branch(value):
                    continue
                dynamical_workflow_generators[name] = value

        return cls(unbound_tasks, subworkflows, dynamical_workflow_generators)


def inherited_classvars(cls: type) -> dict[str, Any]:
    namespace = {}
    for basecls in reversed(cls.mro()):
        namespace.update(vars(basecls))
    return namespace


def workflow(cls):
    """Class decorator for workflows.

    Example::

      @workflow
      class MyClass:
          a = tb.var()

          ...
    """
    # Gather all of the input variables to one dictionary
    inputvars = {}
    for name, value in inherited_classvars(cls).items():
        if isinstance(value, InputVariable):
            inputvars[name] = value

    cls._inputvars = inputvars

    # Define constructor for the Workflow class
    def constructor(self, **kwargs):
        self.inputs = kwargs

        self._rn = None

        names = set(inputvars)

        for name in names:
            if name in kwargs:
                value = kwargs[name]
            else:
                value = inputvars[name]._default
                if value is _no_default:
                    raise TypeError(
                        f'Workflow missing required keyword argument: {name}'
                    )

            if not getattr(value, '_is_tb_workflow', None):
                # We want all workflow inputs to be "future"-ish.
                # Therefore:
                #  * Given a "future"-ish value, we do nothing in particular.
                #  * Given a concrete value, we store Input(value).
                value = Input(value)

            setattr(self, name, value)

        keys = set(kwargs)
        extra_variables = keys.difference(names)
        if extra_variables:
            raise TBUserError(
                f'Unrecognized variables to workflow {extra_variables}'
            )

    cls._is_tb_workflow = True
    cls.__init__ = constructor

    # Create the default branch
    cls._branches = BranchSpecification.from_workflow_class(cls)

    def get_branch(self):
        branch = self._rn.state._current_branch
        if branch not in self._branches:
            raise TBUserError(f"Branch {branch} doesn't exist.")
        return self._branches[branch]

    # Define a wrapper for dynamical workflow generators
    def get_dynamical_workflow_generators(self):
        branch = get_branch(self)
        return branch.dynamical_workflow_generators

    cls._dynamical_workflow_generators = property(
        get_dynamical_workflow_generators
    )

    # Define a wrapper for subworkflows
    def get_subworkflows(self):
        branch = get_branch(self)
        return branch.subworkflows

    cls._subworkflows = property(get_subworkflows)

    # Define a wrapper for unbound tasks
    def get_unbound_tasks(self):
        branch = get_branch(self)
        return branch.unbound_tasks

    cls._unbound_tasks = property(get_unbound_tasks)

    # Define a wrapper for unbound tasks
    def get_external_tasks(self):
        externals = {}
        for branch in self._branches.values():
            for task_name, task in branch.unbound_tasks.items():
                if task.external:
                    externals[task_name] = task
        return externals

    cls._external = property(get_external_tasks)

    def __repr__(self):
        clsname = type(self).__name__
        vartext = ', '.join(
            '{}={}'.format(varname, getattr(self, varname))
            for varname in sorted(self._inputvars)
        )
        return f'<{clsname}({vartext})>'

    def _Phi(self, **kwargs):
        phi = Phi(_rn=self._rn, **kwargs)
        return phi

    cls.Phi = _Phi

    cls.__repr__ = __repr__

    def tb_encode(self):
        return {name: getattr(self, name) for name in self._inputvars}

    @classmethod
    def tb_decode(cls, data):
        assert set(data) == set(inputvars)
        return cls(**data)

    cls.tb_encode = tb_encode
    cls.tb_decode = tb_decode

    return cls


class JSONCodec(ABC):
    """Abstract class for the Encoder/decoder for custom types.

    Taskblaster can encode and decode only specific types.
    A plugin can provide a custom implementation of this class
    to support additional types.
    """

    @abstractmethod
    def decode(self, dct: Dict[str, Any]) -> Any:
        """Decode dictionary generated by encode into object."""

    @abstractmethod
    def encode(self, obj: Any) -> Dict[str, Any]:
        """Encode object as dictionary.

        This should raise TypeError for types that cannot be encoded."""


def mpi(_meth=None, min_cores=None, max_cores=None):
    """Decorator for task implementations that require MPI.

    With multiple MPI subworkers, tasks cannot use MPI world
    as that would cause deadlocks among subworkers.

    Those tasks should instead bear this decorator,
    which causes the taskblaster worker to pass an MPI context
    to the task::

        @mpi
        def mytask(mpi, a):
             print(mpi.comm.rank)

    Tasks that need to use a communicator which is not MPI world
    world communicator may need to access a taskblaster worker's
    communicator.

    Additionally, one may also specify min_cores and max_cores parameters
    to guard code against accidental parallel, or accidental serial
    execution::

        @mpi(max_cores=1)
        def my_serial_task(mpi, a):
            pass  # Always serial, will cause error if attempted to in parallel

        @mpi(min_cores=64)
        def my_parallel_task(mpi, a):
            pass  # Always at least 64 cores. Will cause error otherwise.


    """
    # Allow the decorator to be used either with or without
    # parameters
    if _meth is not None:
        # @tb.mpi is used without parenthesis, core specifications are None
        return _mpi(_meth)
    else:
        return functools.partial(
            _mpi, min_cores=min_cores, max_cores=max_cores
        )


def _mpi(func, min_cores=None, max_cores=None):
    func._tb_mpi = True
    func._tb_min_cores = min_cores
    func._tb_max_cores = max_cores

    @functools.wraps(func)
    def wrapper(*args, mpi=None, **kwargs):
        """Initialize MPI communicators and call the decorated function."""
        # (We should probably use a shortcut for building the task context.)
        from taskblaster.repository import Repository
        from taskblaster.worker import TaskContext

        repo = Repository.find()
        mpi_world = repo.mpi_world()
        if mpi is None:
            usercomm = mpi_world.usercomm()
            mpi = TaskContext(usercomm, mpi_world, 'main')
        if func._tb_min_cores and mpi.comm.size < func._tb_min_cores:
            raise TBUserError(
                'Trying to run a job with too few MPI cores.'
                f'Have {mpi.comm.size}, needed {func._tb_min_cores}.'
            )
        if func._tb_max_cores and mpi.comm.size > func._tb_max_cores:
            raise TBUserError(
                'Trying to run a job with too many MPI cores.'
                f'Have {mpi.comm.size}, max {func._tb_max_cores}.'
            )
        return func(*args, mpi=mpi, **kwargs)

    return wrapper


def context(*contextvars):
    """Decorator for injecting runtime context into tasks.

    Usage:

      @tb.context
      def myfunction(context, potatoes, onions):
          ...

    The :class:`TaskContext` object provides access to runtime
    context such as MPI communicator, and a worker will automatically
    pass that information as the context parameter in the example above.

    To pass specific context variables use::

      @tb.context('comm')
      def myfunction(comm, potatoes, onions):
          ...

    """

    def decorator(func):
        func._tb_context = set(contextvars)
        return func

    # Directly decorated as @tb.context, so we apply decorator right away:
    if len(contextvars) == 1 and callable(contextvars[0]):
        func = contextvars[0]
        contextvars = {'context'}
        return decorator(func)

    return decorator


@mpi(max_cores=1)
def external_file_task(path, mpi):
    print('Setting up external file', path)
    path = Path(path)  # XXX taskblaster does not support jsonin Path objects
    target = Path(path.name)
    print('Copying file', path, 'to', target)
    shutil.copy(path, target)
    return ExternalFile(target).hashed()


class WorkerContext:
    def __init__(self, worker):
        self._worker = worker

    @property
    def comm(self):
        """This worker's MPI communicator object as defined by plugin."""
        return self._worker.comm.usercomm()

    @property
    def tb_comm(self):
        """Taskblaster wrapper for MPI communicator for this worker."""
        return self._worker.comm

    @property
    def rules(self):
        """Worker configuration."""
        return self._worker.rules


TB_STRICT = os.environ.get('TB_STRICT')


__all__ = [
    '_if',
    'actions',
    'branch',
    'define',
    'dynamical_workflow_generator',
    'external_file_task',
    'fixedpoint',
    'InputVariable',
    'JSONCodec',
    'jump',
    'mpi',
    'node',
    'parametrize_glob',
    'Phi',
    'subworkflow',
    'TBUserError',
    'TaskView',
    'task',
    'totree',
    'UserWorkflowError',
    'var',
    'workflow',
]
