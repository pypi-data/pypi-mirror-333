# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/prune/pruner.ipynb.

# %% auto 0
__all__ = ['Pruner']

# %% ../../nbs/prune/pruner.ipynb 2
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch_pruning as tp
from torch_pruning.pruner import function

import numpy as np
import torch
import torch.nn as nn
import pickle
from itertools import cycle
from fastcore.basics import store_attr, listify, true
from ..core.criteria import *
from fastai.vision.all import *


from torch_pruning.pruner.algorithms.scheduler import linear_scheduler
from torch.fx import symbolic_trace

# %% ../../nbs/prune/pruner.ipynb 4
class Pruner():
    def __init__(self, model, pruning_ratio, context, criteria, schedule=linear_scheduler, ignored_layers=None, example_inputs=torch.randn(1, 3, 224, 224), *args, **kwargs):
        store_attr()
        self.num_heads = {}
        if not self.ignored_layers: self.get_ignored_layers(self.model)
        if self.pruning_ratio>1: self.pruning_ratio = self.pruning_ratio/100 
        self.pruner = tp.pruner.MetaPruner(
        self.model,
        example_inputs=self.example_inputs.to(next(self.model.parameters()).device),
        importance=self.group_importance,
        pruning_ratio=self.pruning_ratio, 
        ignored_layers=self.ignored_layers,
        global_pruning=True if self.context=='global' else False,
        num_heads = self.num_heads,
        iterative_pruning_ratio_scheduler=self.schedule,
        *args, 
        **kwargs
        )
          
    def prune_model(self):
        self.pruner.step()
        self.restore_attention_layers()


    def get_linear_layers_to_ignore(self, model):
        traced = symbolic_trace(model)
        for node in traced.graph.nodes:
            if node.op == "output":  # Identifier la sortie
                for input_node in node.all_input_nodes:
                    if input_node.target:  # Trouver la couche correspondante
                        module = dict(model.named_modules()).get(input_node.target)
                        if isinstance(module, torch.nn.Linear):
                            self.ignored_layers.append(module)
                            print(f"Ignoring output layer: {module}")


    def get_attention_layers_to_ignore(self, model):
        for module in model.modules():
            if hasattr(module, 'num_heads'):
                if hasattr(module, 'qkv'):
                    self.ignored_layers.append(module.qkv)  # Ajouter à la liste globale
                    self.num_heads[module.qkv] = module.num_heads
                    print(f"Attention layer ignored: {module.qkv}, num_heads={module.num_heads}")
                elif hasattr(module, 'qkv_proj'):
                    self.ignored_layers.append(module.qkv_proj)  # Ajouter à la liste globale
                    self.num_heads[module.qkv_proj] = module.num_heads
                    print(f"Attention layer ignored: {module.qkv_proj}, num_heads={module.num_heads}")

    
    def get_ignored_layers(self, model):
        self.ignored_layers = []
        self.get_linear_layers_to_ignore(model)
        self.get_attention_layers_to_ignore(model)
        print(f"Total ignored layers: {len(self.ignored_layers)}")
    
                
    def restore_attention_layers(self):
        for m in self.model.modules():
            # Attention layers
            if hasattr(m, 'num_heads'):
                if hasattr(m, 'qkv'):
                    m.num_heads = self.num_heads[m.qkv]
                    m.head_dim = m.qkv.out_features // (3 * m.num_heads)
                elif hasattr(m, 'qkv_proj'):
                    m.num_heads = self.num_heads[m.qqkv_projkv]
                    m.head_dim = m.qkv_proj.out_features // (3 * m.num_heads)


    def group_importance(self, group):
        handler_map = {
            function.prune_conv_out_channels: 'filter',
            function.prune_linear_out_channels: 'row',
            function.prune_linear_in_channels: 'column',
            function.prune_conv_in_channels: 'shared_kernel',
            # Additional handlers can be added here
        }
    
        group_imp = []
        group_idxs = []
    
        for i, (dep, idxs) in enumerate(group):
            if dep.handler in handler_map:
                impo = self.criteria(dep.target.module, handler_map.get(dep.handler), squeeze=True)
                group_imp.append(impo)
                group_idxs.append(group[i].root_idxs)
    
        reduced_imp = torch.zeros_like(group_imp[0])
    
        for i, (imp, root_idxs) in enumerate(zip(group_imp, group_idxs)):
            imp = imp.to('cpu')
            reduced_imp = reduced_imp.to('cpu')
            reduced_imp.scatter_add_(0, torch.tensor(root_idxs, device=imp.device), imp)
    
        reduced_imp /= len(group_imp)
    
        return reduced_imp.to(default_device())
