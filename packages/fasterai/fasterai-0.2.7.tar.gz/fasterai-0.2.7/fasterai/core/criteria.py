# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/core/criteria.ipynb.

# %% auto 0
__all__ = ['EPS', 'random', 'large_final', 'squared_final', 'small_final', 'large_init', 'small_init', 'large_init_large_final',
           'small_init_small_final', 'magnitude_increase', 'movement', 'updating_magnitude_increase',
           'updating_movement', 'updating_movmag', 'criterias', 'Reducer', 'Normalizer', 'Criteria',
           'magnitude_criteria', 'init_based_criteria', 'update_based_criteria', 'available_criterias', 'grad_crit']

# %% ../../nbs/core/criteria.ipynb 2
import torch
import torch.nn as nn
import torch.nn.functional as F
from fastcore.basics import *
from fastcore.imports import *
from .granularity import *
from typing import Callable, Optional, Union, List, Tuple
from enum import Enum, auto

# %% ../../nbs/core/criteria.ipynb 6
EPS = torch.finfo(torch.float32).eps

# %% ../../nbs/core/criteria.ipynb 7
class Reducer:
    @staticmethod
    def sum(scores, dim):
        return scores[None].sum(dim=dim, keepdim=True).squeeze(0)
    
    @staticmethod
    def mean(scores, dim):
        return scores[None].mean(dim=dim, keepdim=True).squeeze(0)

# %% ../../nbs/core/criteria.ipynb 8
class Normalizer:
    @staticmethod
    def sum(scores):
        return scores / scores.sum()
    
    @staticmethod
    def standardization(scores):
        return (scores - scores.min()) / (scores.max() - scores.min() + EPS)
    
    @staticmethod
    def mean(scores):
        return scores / scores.mean()
    
    @staticmethod
    def max(scores):
        return scores / scores.max()
    
    @staticmethod
    def gaussian(scores):
        return (scores - scores.mean()) / (scores.std() + EPS)

# %% ../../nbs/core/criteria.ipynb 9
class Criteria():
    def __init__(self, 
                 f:Callable[[torch.Tensor], torch.Tensor],                                         # Function that transforms weights (e.g., torch.abs, torch.square)
                 reducer: Callable = Reducer.mean,                                                 # Method to reduce dimensions ('mean' or 'sum')
                 normalizer: Optional[Callable] = None,                                            # Method to normalize scores (None, 'sum', 'standardization', 'mean', 'max', 'gaussian')
                 needs_init:bool=False,                                                            # Whether this criteria needs the initial weights
                 needs_update:bool=False,                                                          # Whether this criteria needs to track weight updates between iterations
                 output_fn:Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,  # Function to combine current and reference weights
                 return_init=False,                                                                # Whether to return the transformed initial weights instead of final output
    ):
        "Evaluates neural network parameters based on various criteria for pruning"
        store_attr()
        assert (needs_init and needs_update)==False, "The init values will be overwritten by the updating ones."
   
    @torch.no_grad()
    def __call__(self, 
                 m: nn.Module,  # The module to compute scores for
                 g: str,        # Granularity specification
                 squeeze=False  # Whether to squeeze singleton dimensions
    ) -> torch.Tensor:
        "Compute criteria scores for module weights"
        try:
            dim = listify(Granularities.get_dim(m, g))
        except KeyError:
            raise ValueError(f'Invalid granularity "{g}" for module type {type(m).__name__}')
            
        if self.needs_update and hasattr(m, '_old_weights') == False:
            m.register_buffer("_old_weights", m._init_weights.clone()) # If the previous value of weights is not known, take the initial value
            
        wf = self.f(m.weight)
        
        if self.needs_init: wi = self.f(m._init_weights)
        if self.needs_update: wi = self.f(m._old_weights)
        
        if self.output_fn: scores = self.output_fn(wf, wi)
        elif self.return_init: scores = wi
        else: scores = wf
            
        scores = self._rescale(scores)
        if hasattr(m, '_mask'): scores.mul_(m._mask)
        scores = self._reduce(scores, dim)
        scores = self._normalize(scores)
        if squeeze: scores = scores[None].squeeze((0,*dim))
        return scores
    
    def _reduce(self, 
                scores: torch.Tensor,  # Input scores
                dim: Union[int, List[int]]      # Dimensions to reduce
    ) -> torch.Tensor:
        "Reduce scores along specified dimensions"
        return self.reducer(scores, dim)
            
    def _normalize(self, 
                   scores: torch.Tensor # Input scores to normalize
    ) -> torch.Tensor:
        "Normalize scores using the specified method"
        if self.normalizer is None: return scores
        return self.normalizer(scores)

    def _rescale(self, 
                 scores: torch.Tensor # Inpute scores to rescale
    ) -> torch.Tensor:
        "Ensure all scores are positive to maintain pruning behavior"
        min_val = scores.min()
        if min_val < 0:
            scores.add_(-min_val)
        scores.add_(EPS)
        return scores

    def update_weights(self, 
                       m: nn.Module   # Module whose weights should be updated
    ) -> None:
        "Update the reference weights for criteria that track changes"
        if self.needs_update: 
            m._old_weights = m.weight.data.clone() # The current value becomes the old one for the next iteration

# %% ../../nbs/core/criteria.ipynb 11
def magnitude_criteria(transform_fn, **kwargs):
     "Create a criteria based on weight magnitude transformation."
     return Criteria(transform_fn, **kwargs)

# %% ../../nbs/core/criteria.ipynb 13
random = magnitude_criteria(torch.randn_like)

# %% ../../nbs/core/criteria.ipynb 16
large_final = magnitude_criteria(torch.abs)

# %% ../../nbs/core/criteria.ipynb 19
squared_final = magnitude_criteria(torch.square)

# %% ../../nbs/core/criteria.ipynb 22
small_final = magnitude_criteria(compose(torch.abs, torch.neg))

# %% ../../nbs/core/criteria.ipynb 25
def init_based_criteria(transform_fn, output_fn=None, return_init=False, **kwargs):
     "Create a criteria that compares current weights to initial weights."
     return Criteria(transform_fn, needs_init=True, output_fn=output_fn, return_init=return_init, **kwargs)

# %% ../../nbs/core/criteria.ipynb 27
large_init = init_based_criteria(torch.abs, return_init=True)

# %% ../../nbs/core/criteria.ipynb 30
small_init = init_based_criteria(compose(torch.abs, torch.neg), return_init=True)

# %% ../../nbs/core/criteria.ipynb 33
large_init_large_final = init_based_criteria(torch.abs, output_fn=torch.min)

# %% ../../nbs/core/criteria.ipynb 36
small_init_small_final = init_based_criteria(torch.abs, output_fn=lambda x,y: torch.neg(torch.max(x,y)))

# %% ../../nbs/core/criteria.ipynb 39
magnitude_increase = init_based_criteria(torch.abs, output_fn= torch.sub)

# %% ../../nbs/core/criteria.ipynb 42
movement = init_based_criteria(noop, output_fn= lambda x,y: torch.abs(torch.sub(x,y)))

# %% ../../nbs/core/criteria.ipynb 47
def update_based_criteria(transform_fn, output_fn=None, **kwargs):
     "Create a criteria that compares current weights to previous iteration weights."
     return Criteria(transform_fn, needs_update=True, output_fn=output_fn, **kwargs)

# %% ../../nbs/core/criteria.ipynb 50
updating_magnitude_increase = update_based_criteria(torch.abs, output_fn= lambda x,y: torch.sub(x,y))

# %% ../../nbs/core/criteria.ipynb 53
updating_movement = update_based_criteria(noop, output_fn= lambda x,y: torch.abs(torch.sub(x,y)))

# %% ../../nbs/core/criteria.ipynb 56
updating_movmag = update_based_criteria(noop, output_fn=lambda x,y: torch.abs(torch.mul(x, torch.sub(x,y))))

# %% ../../nbs/core/criteria.ipynb 58
criterias = (
    'random',              # Random scores
    'large_final',         # Large absolute weight values
    'small_final',         # Small absolute weight values
    'squared_final',       # Squared weight values
    'large_init',          # Large initial weight values
    'small_init',          # Small initial weight values
    'large_init_large_final',  # Minimum of initial and final magnitude
    'small_init_small_final',  # Maximum of initial and final magnitude (negated)
    'magnitude_increase',      # Increase in magnitude from init
    'movement',                # Absolute change from init
    'updating_magnitude_increase',  # Increase in magnitude from previous step
    'updating_movement',           # Absolute change from previous step
    'movmag',                      # Movement * magnitude from init
    'updating_movmag'              # Movement * magnitude from previous step
 )

def available_criterias():
    "Return the list of available criteria names"
    return criterias

# %% ../../nbs/core/criteria.ipynb 78
def grad_crit(m, g):
    if g in granularities[m.__class__.__name__]: 
        dim = granularities[m.__class__.__name__][g]
        if m.weight.grad is not None:
            return (m.weight*m.weight.grad)[None].pow(2).mean(dim=dim, keepdim=True).squeeze(0)
        else: 
            return m.weight[None].pow(2).mean(dim=dim, keepdim=True).squeeze(0)
    else: raise NameError('Invalid Granularity') 
