# from curses import set_tabsize
from pathlib import Path
from pdb import set_trace

import numpy as np
import xarray as xr

from gfatpy.lidar.utils.file_manager import filename2info, search_dc

# from gfatpy.lidar.preprocessing.lidar_gluing_bravo_aranda import gluing
from gfatpy.lidar.preprocessing.gluing_proportional import gluing
from gfatpy.lidar.preprocessing.lidar_preprocessing_tools import ff_2D_overlap_from_channels
from gfatpy.lidar.utils.utils import LIDAR_INFO
from gfatpy.utils import calibration
from gfatpy.utils.io import read_yaml_from_info

# from .lidar_merge import apply_polarization_merge
# from .lidar_preprocessing_tools import *  # TODO: Remove wildcard importations within modules

# warnings.filterwarnings("ignore")

__author__ = "Bravo-Aranda, Juan Antonio"
__license__ = "GPL"
__version__ = "1.0"
__maintainer__ = "Bravo-Aranda, Juan Antonio"
__email__ = "jabravo@ugr.es"
__status__ = "Production"


""" DEFAULT AUXILIAR INFO
"""
# Root Directory (in NASGFAT)  according to operative system
""" LIDAR PREPROCESSING
"""


def preprocess(
    file_or_path: Path | str,
    channels: list[str] | None = None,
    crop_ranges: tuple[float, float] | None = (0, 20000),
    background_ranges: tuple[float, float] | None = None,
    # TODO: Use stage flags dc, dt, bz. Include add corrected in Dataset flag for False in each
    apply_dc: bool = True,
    apply_dt: bool = True,
    apply_bg: bool = True,
    apply_bz: bool = True,
    apply_ov: bool = False,
    save_dc: bool = False,
    save_bg: bool = False,
    gluing_products: bool = False,
    force_dc_in_session=False,    
    overlap_path: Path | str | None = None,
) -> xr.Dataset:
    """It preprocesses lidar netcdf file. It can optionally apply dark-measurement, deadtime, background, zero-bin, and overlap correction and calculate gluing products.

    Args:

         -file_or_path (Path | str): A relative (of where is executing) or absolute path to an nc file generated by the linc. If not exists will try to search it in NAS
         -channels (list[str] | None, optional): List of string channels, if not provided will return all available channels. Defaults to None.
         -crop_ranges (tuple[float, float] | None, optional): outpur range. Defaults to (0, 20000).
         -background_ranges (tuple[float, float] | None, optional): Range where background correction will be performed. Defaults to None.
         -apply_dc (bool, optional): it allows the dark measurement correction. Defaults to True.
         -apply_dt (bool, optional): it allows the dead time correction. Defaults to True.
         -apply_bg (bool, optional): it allows the background correction. Defaults to True.
         -apply_bz (bool, optional): it allows the zero bien correction. Defaults to True.
         -apply_ov (bool, optional): it allows the overlap correction. Defaults to False.
         -save_dc (bool, optional): it saves the dark measurement in the xr.Dataset. Defaults to False.
         -save_bg (bool, optional): it saves the background in the xr.Dataset. Defaults to False.
         -gluing_products (bool, optional): it performs the detection-mode gluing. Defaults to False.
         -force_dc_in_session (bool, optional): it forces to use the dark measurement linked to this measurement period. If not found, the correction cannot be performed. Defaults to False.
         -dir (Path, None): Directory where lidar folder tree starts.
         -overlap_path (Path, str, None): Path to the overlap file. Defaults to None. If None and apply_ov is TRUE, the overlap file is searched in the dir/<lidar_name>/QA/overlap/.
    
    Returns:

         -xr.Dataset: xarray.Dataset with the lidar data and information.
    """

    _p = Path(file_or_path)
    if not _p.exists():
        raise ValueError(f"{file_or_path} not found")

    with xr.open_dataset(_p, chunks={}, engine='netcdf4') as _nc:
        dataset = _nc
    dataset = drop_unwanted_channels(dataset, channels=channels)

    #Get info from filename
    lidar_nick, _, _, _, _, date  = filename2info(_p.name)

    global INFO
    INFO = read_yaml_from_info(lidar_nick, date)

    #If header is defined in INFO, its fields are used to update the dataset. Variables and attributes can be updated. See example on info_alh_20221117.yml
    if 'header' in INFO.keys():
        update_from_info(dataset, INFO['header'])

    if apply_dc:        
        dataset = apply_dark_current_correction(
            dataset,
            rs_path=_p,
            save_dc=save_dc,
            force_dc_in_session=force_dc_in_session,
        )
    else:
        dataset.attrs["dc_corrected"] = str(False)

    if apply_dt:
        dataset = apply_dead_time_correction(
            dataset
        )  # TODO implement search_dt() in the same way of search_dc
    else:
        dataset.attrs["dt_corrected"] = str(False)

    if apply_bg:
        dataset = apply_background_correction(
            dataset, background_ranges=background_ranges, save_bg=save_bg
        )
    else:
        dataset.attrs["bg_corrected"] = str(False)

    if apply_bz:
        dataset = apply_bin_zero_correction(dataset, rs_path=_p)
    else:
        dataset.attrs["bz_corrected"] = str(False)

    if apply_ov:
        dataset = apply_overlap_correction(dataset, overlap_path)
        dataset.attrs["ov_corrected"] = "True"
    else:
        dataset.attrs["ov_corrected"] = "False"

    dataset = apply_crop_ranges_correction(dataset, crop_ranges=crop_ranges)

    if gluing_products:
        dataset = apply_detection_mode_merge(dataset)
    
    dataset = add_height(dataset)

    return dataset

def update_from_info(dataset: xr.Dataset, header: dict):
    """Update dataset variables and attributes from INFO header.

    Args:

        - dataset (xr.Dataset): Dataset to be updated.
        - header (dict): Header from INFO.

    Raises:

        - RuntimeError: Variables or attributes could not be updated.
        - RuntimeError: Attributes could not be updated.

    Returns:

        - xr.Dataset: Updated dataset.
    """        
    for key_ in header.keys():
        if key_ in [* dataset.variables.keys()]:                        
            try:
                # Replace dataset[key_] with header[key_] using the same dimensions and coordinates
                dataset[key_] = xr.DataArray(header[key_], dims=dataset[key_].dims, coords=dataset[key_].coords)
            except Exception as e:
                raise RuntimeError(f"Could not update variable {key_} from INFO. Error: {e}")
        if key_ in [* dataset.attrs.keys()]:  
            try:
                dataset.attrs[key_] = header[key_]
            except Exception as e:
                raise RuntimeError(f"Could not update attribute {key_} from INFO. Error: {e}")            
    return dataset

def drop_unwanted_channels(
    dataset: xr.Dataset, channels: list[str] | None
) -> xr.Dataset:
    """Drop unwanted channels from dataset.

    Args:

        - dataset (xr.Dataset): Dataset to be updated.
        - channels (list[str] | None): List of channels to be removed. Defaults to None meaning all channels are kept.

    Returns:

        - xr.Dataset: Updated dataset.
    """    
    if channels is None:
        return dataset

    channel_names = list(filter(lambda c: c not in channels, dataset.channel.values))

    dataset = dataset.drop_vars(
        map(lambda c: f"signal_{c}", channel_names), errors="raise"
    )

    dataset = dataset.drop_sel(channel=channel_names, errors="raise")

    return dataset

def add_height(dataset: xr.Dataset) -> xr.Dataset:
    """Add height coordinate to dataset.

    Args:

        - dataset (xr.Dataset): Dataset to be updated.

    Raises:

        - RuntimeError: Zenithal angle is not constant in time. Cannot add height coordinate.

    Returns:

        - xr.Dataset: Updated dataset.
    """    
    #Add height because of zenithal angle
    if "zenithal_angle" in [*dataset.variables.keys()]:
        if dataset["zenithal_angle"].values.all():         
            zenithal_angle = np.deg2rad(dataset["zenithal_angle"].values[0])
            if zenithal_angle != 0:
                dataset["height"] = dataset["range"] * np.cos(zenithal_angle)
            else:
                dataset["height"] = dataset["range"].copy()
        else:
            raise RuntimeError("Zenithal angle is not constant in time. Cannot add height coordinate.")
    return dataset

def apply_dark_current_correction(
    dataset: xr.Dataset, rs_path: Path, save_dc: bool = False, force_dc_in_session=False
) -> xr.Dataset:
    """Apply dark current correction to dataset.

    Args:

        - dataset (xr.Dataset): Dataset to be corrected.
        - rs_path (Path): Path to the raw data.
        - save_dc (bool, optional): Flag to save the dark current in the dataset. Defaults to False.
        - force_dc_in_session (bool, optional): Flag to force the use of the dark current linked to this measurement period. If not found, the correction cannot be performed. Defaults to False.

    Raises:

        - Warning: Dark current std is too high for an specified channel. Dark measurement shall be not appropiated.

    Returns:

        - xr.Dataset: Dataset with dark current corrected.
    """    
    groups = calibration.split_continous_measurements(dataset.time.values)
    channels = dataset.channel.values

    analog_channels: list[str] = list(filter(lambda c: c.endswith("a"), channels))

    for group in groups:
        dc_path = search_dc( rs_path, session_period=group[[0, -1]], force_dc_in_session=force_dc_in_session, )
        dc = xr.open_dataset(dc_path)

        lower_idx = np.where(dataset.time == group[0])[0][0]
        upper_idx = np.where(dataset.time == group[-1])[0][0] + 1

        for channel in analog_channels:
            signal_str = f"signal_{channel}"

            if dc[signal_str].sel(range=slice(0, 5000)).std("range").std("time") > 0.07:
                raise Warning(
                    f"Dark current std is too high for channel {channel} in {dc_path}. Check if DC was measured correctly."
                )

            dc_mean = dc[signal_str].values.mean(0)
            dataset[signal_str][lower_idx:upper_idx] -= dc_mean

            if save_dc:
                if (
                    f"dc_{channel}" not in list(dataset.variables.keys())
                    and lower_idx == 0
                ):
                    dataset[f"dc_{channel}"] = dataset[signal_str] * np.nan

                dataset[f"dc_{channel}"][lower_idx:upper_idx] = dc_mean

    dataset.attrs["dc_corrected"] = str(True)

    return dataset


def apply_dead_time_correction(dataset: xr.Dataset) -> xr.Dataset:
    """Apply dead time correction to dataset.

    Args:

        - dataset (xr.Dataset): Dataset to be corrected.

    Raises:

        - ValueError: No dead time value defined in INFO->{lidar_name}->{channel}.
        - ValueError: No dead time value defined in INFO->{lidar_name}.

    Returns:

        - xr.Dataset: Dataset with dead time corrected.
    """    
    # dt_path = search_dt(rs_path, session_period=dataset.time.values[[0,-1]]) #
    # dt_dict = open_dataset(dt_path) #TODO

    lidar_name = dataset.attrs["system"].lower()
    try:
        dt_dict = {
            key: value["dead_time_ns"]
            for (key, value) in INFO["channels"].items()
            if value.get("dead_time_ns", False)
        }
    except Exception:
        raise ValueError(f"No dead time value defined in INFO->{lidar_name}].")

    photocounting_channels: list[str] = list(
        filter(lambda c: c.endswith("p"), dataset.channel.values)
    )

    for channel in photocounting_channels:
        # tau from ns to us
        try:
            tau_us = dt_dict[channel] * 1e-3
        except KeyError:
            raise ValueError(  # TODO
                f"No dead time value defined in INFO->{lidar_name}->{channel}."
            )

        signal_str = f"signal_{channel}"

        # Eq 4 [D'Amico et al., 2016]
        # No infinites nor negative values
        # condition = np.logical_and(~np.isinf(dataset[signal_str]), dataset[signal_str] > 0)
        # dataset[signal_str] = dataset[signal_str].where(condition)/ ( 1 - dataset[signal_str].where(condition) * tau_us )

        dataset[signal_str] = dataset[signal_str] / (1 - dataset[signal_str] * tau_us)
        dataset.attrs["dt_corrected"] = str(True)

    return dataset


def apply_background_correction(
    dataset: xr.Dataset,
    background_ranges: tuple[float, float] | None = None,
    save_bg: bool = False,
) -> xr.Dataset:
    """Apply background correction to dataset.

    Args:

        - dataset (xr.Dataset): Dataset to be corrected.
        - background_ranges (tuple[float, float] | None, optional): Range where background average will be performed for each signal. This value is subtracted to the whole profile. Defaults to None.
        - save_bg (bool, optional): Flag to save the background in the dataset. Defaults to False.

    Raises:

        - ValueError: background_ranges should be in order (min, max).

    Returns:

        - xr.Dataset: Dataset with background corrected.
    """    
    if background_ranges is None:
        background_ranges = (
            dataset.attrs["BCK_MIN_ALT"],
            dataset.attrs["BCK_MAX_ALT"],
        )

    if background_ranges[1] <= background_ranges[0]:
        raise ValueError("background_ranges should be in order (min, max)")

    ranges_between = (background_ranges[0] < dataset.range) & (
        dataset.range < background_ranges[1]
    )
    channels: list[str] = dataset.channel.values
    for channel in channels:
        signal_str = f"signal_{channel}"
        try:
            background = dataset[signal_str].loc[:, ranges_between].mean(axis=1)
        except:
            background = np.ones(1)
        dataset[signal_str] -= background

        if save_bg:
            dataset[f"bg_{channel}"] = background

    dataset.attrs["bg_corrected"] = str(True)

    return dataset


def apply_bin_zero_correction(dataset: xr.Dataset, rs_path: Path) -> xr.Dataset:
    """Apply zero bin correction to dataset.

    Args:

        - dataset (xr.Dataset): Dataset to be corrected.
        - rs_path (Path): Path to the raw data.

    Raises:

        - ValueError: No bin zero value defined in INFO->{lidar_name}.

    Returns:

        - xr.Dataset: Dataset with zero bin corrected.
    """    
    # bz_path = search_bz(rs_path, session_period=dataset.time.values[[0,-1]]) #
    # bz_dict = open_dataset(bz_path) #TODO
    bz_dict = None

    if bz_dict is None:
        lidar_name = dataset.attrs["system"].lower()
        try:
            bz_dict = {
                key: value["bin_zero"]
                for (key, value) in INFO["channels"].items()
            }
        except Exception:
            raise ValueError(f"No bin zero value defined in INFO->{lidar_name}.")

    channels: list[str] = dataset.channel.values
    for channel in channels:
        signal_str = f"signal_{channel}"
        dataset[signal_str] = dataset[signal_str].shift(
            range=-bz_dict[channel], fill_value=0.0
        )

    dataset.attrs["bz_corrected"] = str(True)

    return dataset


def apply_overlap_correction(
    dataset: xr.Dataset,
    ff_overlap_path: Path | str | None = None,
    nf_overlap_path: Path | str | None = None,
) -> xr.Dataset:
    """Apply overlap correction to dataset.

    Args:

        - dataset (xr.Dataset): Dataset to be corrected.
        - overlap_path (Path): Path to the overlap file.

    Returns:

        - xr.Dataset: Dataset with overlap corrected.
    """

    def apply_correction(
        dataset: xr.Dataset,
        overlap: xr.DataArray,
        channels2correct: list[str],
        overlap_channel: str,
    ) -> xr.Dataset:
        
        try:
            for channel_ in channels2correct:
                if channel_ in dataset.channel.values:
                    signal_str = f"signal_{channel_}"

                    # Perform overlap correction
                    dataset[signal_str] = dataset[signal_str] / overlap
                    dataset[signal_str].attrs["overlap_applied"] = overlap_channel
                    dataset["overlap_corrected"][dataset.channel.values == channel_] = 1
        except:
            raise Warning("Could not apply overlap correction")
        return dataset   

    lidar_name = dataset.attrs["system"].lower()
    overlap_channels = INFO['overlap_channels']
    linked_channels = INFO['overlap_linked_channels']

    # New array to store if overlap correction was applied
    overlap_corrected = xr.DataArray(
        np.zeros(dataset.channel.size),
        dims=("channel"),
        coords={"channel": dataset.channel},
    )
    dataset["overlap_corrected"] = overlap_corrected

    # Apply overlap correction for far-field channels
    #TODO: Move to LIDAR_INFO

    # Load overlap dataset
    if ff_overlap_path is not None:
        overlap = xr.open_dataarray(ff_overlap_path)        
    else:
        overlap = None

    for channel_ in linked_channels.keys():
        #Check channel_ in dataset

        if channel_ not in dataset.channel.values:
            continue
        
        # Select overlap array for channel
        if overlap is not None:
            if channel_ in overlap.channel.values:                
                overlap_ = overlap.sel(channel=channel_)
            else:
                raise ValueError(f"Channel {channel_} not found in overlap file")
        else:                        
            nf_channel_ = overlap_channels[channel_]
            if not nf_channel_ in dataset.channel.values:
                raise ValueError(f"Channel {nf_channel_} not found in dataset. Please include near-field channel in dataset to retrieve overlap.")
            overlap_ = ff_2D_overlap_from_channels(dataset, channel_ff=channel_, channel_nf=nf_channel_)
        # Check size of arrays
        if dataset.range.size != overlap_.range.size:
            # Adapt size of overlap array to dataset
            overlap_ = overlap_.interp(range=dataset.range, method="linear")

        # Save overlap array
        if overlap_.values.any():
            # Apply overlap correction
            dataset = apply_correction(
                dataset, overlap_, linked_channels[channel_], overlap_channel=channel_
            )

            dataset[f"overlap_{channel_}"] = overlap_

    # Apply overlap correction for far-field channels
    if nf_overlap_path is not None:
        linked_channels = {
            "355npa": [
                "355npa",
                "355npp",
                "355nsa",
                "355nsp",
                "387nta",
                "387ntp",
                "408nta",
                "408ntp",
            ],
            "532npa": ["532npa", "532npp", "607nta", "607ntp"],
        }
        # Load overlap dataset
        overlap = xr.open_dataarray(nf_overlap_path)

        for channel_ in overlap.channel.values:
            # Select overlap array for channel
            overlap_ = overlap.sel(channel=channel_)

            # Check size of arrays
            if dataset.range.size != overlap_.range.size:
                # Adapt size of overlap array to dataset
                overlap_ = overlap_.interp(range=dataset.range, method="linear")

            # Apply overlap correction
            dataset = apply_correction(
                dataset, overlap_, linked_channels[channel_], overlap_channel=channel_
            )

            # Save overlap array
            if dataset["overlap_corrected"].values.any():
                dataset[f"overlap_{channel_}"] = overlap_
    return dataset


def apply_crop_ranges_correction(
    dataset: xr.Dataset, crop_ranges: tuple[float, float] | None = (0, 20000)
) -> xr.Dataset:
    """It crops the dataset range dimension to `crop_ranges` values.

    Args:

        - dataset (xr.Dataset): Dataset to be cropped.
        - crop_ranges (tuple[float, float] | None, optional): Mininum and maximum ranges. Defaults to (0, 20000).

    Raises:

        - ValueError: crop_ranges should be in order (min, max).

    Returns:

        - xr.Dataset: Dataset with range cropped.
    """    
    # TODO: Apply crop ranges. With dataset.sel(range=slice(*crop_ranges))

    if crop_ranges is None:
        return dataset

    if crop_ranges[0] > crop_ranges[-1]:
        raise ValueError("crop_ranges should be in order (min, max)")

    dataset = dataset.sel(range=slice(*crop_ranges))

    return dataset


def apply_detection_mode_merge(dataset: xr.Dataset)-> xr.Dataset:
    """It performs the detection-mode gluing.

    Args:
        dataset (xr.Dataset): Dataset to be merged.

    Returns:
        xr.Dataset: Merged dataset.
    """    
    LIDAR_INFO["metadata"]["code_telescope_str2number"]
    LIDAR_INFO["metadata"]["code_mode_str2number"]

    channels_pc: list[str] = list(
        filter(lambda c: c.endswith("p"), dataset.channel.values)
    )

    # wavelength, telescope, polarization, mode, channel_name = [], [], [], [], []
    range_m = dataset["range"].values

    glued_list: list[dict] = []

    glued_list: list[dict] = []

    for channel_pc in channels_pc:
        channel_an = f"{channel_pc[0:-1]}a"

        if f"signal_{channel_an}" not in list(dataset.variables.keys()):
            continue

        # rcs_an = signal_to_rcs(dataset[f"signal_{channel_an}"], range_m)  # type: ignore
        # rcs_pc = signal_to_rcs(dataset[f"signal_{channel_pc}"], range_m)  # type: ignore

        # if type(rcs_an) != xr.DataArray or type(rcs_pc) != xr.DataArray:
        #     raise TypeError("RCS must be xarray.DataArray")

        signal_gl = gluing(
            dataset[f"signal_{channel_an}"], dataset[f"signal_{channel_pc}"]
        )

        channel_sel = dataset.sel(channel=channel_an)

        glued_list.append(
            {
                "name": f"{channel_pc[0:-1]}g",
                "signal": signal_gl,
                "polarization": channel_sel.polarization,
                "telescope": channel_sel.telescope,
                "wavelength": channel_sel.telescope,
            }
        )

    glued_signal = {
        f"signal_{glued['name']}": (["time", "range"], glued["signal"])
        for glued in glued_list
    }

    other_var = {
        "wavelength": (["channel"], [g["wavelength"] for g in glued_list]),
        "polarization": (["channel"], [g["polarization"] for g in glued_list]),
        "telescope": (["channel"], [g["telescope"] for g in glued_list]),
        "bin_shift": (["channel"], [0 for _ in glued_list]),
    }

    glued_dataset = xr.Dataset(
        glued_signal | other_var,
        coords={
            "range": range_m,
            "time": dataset["time"].values,
            "channel": list(map(lambda i: i["name"], glued_list)),
        },
    )

    return xr.merge([dataset, glued_dataset])
