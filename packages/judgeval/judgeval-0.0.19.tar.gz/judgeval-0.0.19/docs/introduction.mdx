---
title: Introduction to Judgeval
description: ""
---

{/*
<img
  className="block dark:hidden"
  src="/images/checks-passed.png"
  alt="Hero Light"
/>
<img
  className="hidden dark:block"
  src="/images/checks-passed.png"
  alt="Hero Dark"
/>
*/}

{/*
## Setting up

The first step to world-class documentation is setting up your editing environments.

<CardGroup cols={2}>
  <Card
    title="Edit Your Docs"
    icon="pen-to-square"
    href="https://mintlify.com/docs/quickstart"
  >
    Get your docs set up locally for easy development
  </Card>
  <Card
    title="Preview Changes"
    icon="image"
    href="https://mintlify.com/docs/development"
  >
    Preview your changes before you push to make sure they're perfect
  </Card>
</CardGroup>

## Make it yours

Update your docs to your brand and add valuable content for the best user conversion.

<CardGroup cols={2}>
  <Card
    title="Customize Style"
    icon="palette"
    href="https://mintlify.com/docs/settings/global"
  >
    Customize your docs to your company's colors and brands
  </Card>
  <Card
    title="Reference APIs"
    icon="code"
    href="https://mintlify.com/docs/api-playground/openapi"
  >
    Automatically generate endpoints from an OpenAPI spec
  </Card>
  <Card
    title="Add Components"
    icon="screwdriver-wrench"
    href="https://mintlify.com/docs/content/components/accordions"
  >
    Build interactive features and designs to guide your users
  </Card>
  <Card
    title="Get Inspiration"
    icon="stars"
    href="https://mintlify.com/customers"
  >
    Check out our showcase of our favorite documentation
  </Card>
</CardGroup>

*/}

Judgeval is an evaluation library for **multi-step LLM systems**. 
Judgeval is designed for AI teams to easily benchmark and iterate on their LLM apps and was designed to:
- Offer a development and production **quality control layer** for multi-step LLM applications, especially for **agentic systems**.
- **Plug-and-evaluate** LLM systems with 10+ research-backed metrics including hallucination detection, RAG retriever quality, and more.
- Construct powerful custom evaluation pipelines for your LLM systems.
- Monitor LLM systems in production using state-of-the-art **real-time evaluation foundation models**.

Judgeval integrates natively with the **Judgment Labs Platform**, allowing you to [evaluate](/evaluation/introduction), regression test, 
and [monitor](/monitoring/introduction) LLM applications in the cloud.

Judgeval was built by a passionate team of LLM researchers from **Stanford, Datadog, and Together AI**.

Click [here](/getting_started) to get started.
