---
title: ToolCorrectness
description: ""
---

The `ToolCorrectness` scorer is a default scorer that checks whether your LLM correctly calls and uses tools. 
In practice, this allows you to assess the quality of an LLM agent's tool choice and applied use of tools.

## Required Fields

To run the `ToolCorrectness` scorer, you must include the following fields in your `Example`:
- `input`
- `actual_output`
- `tools_called`
- `expected_output`

## Scorer Breakdown

The tool correctness score is calculated via the fraction of the total number of tools called that are correct.

$$
\text{Tool Correctness} = \frac{\text{Number of Correct Tools Called}}{\text{Total Number of Tools Called}}
$$

TODO add more docs here regarding tool ordering, exact match, or even correct tool use aside from calling tools.

## Sample Implementation 

```python tool_correctness.py
from judgeval import JudgmentClient
from judgeval.data import Example
from judgeval.scorers import ToolCorrectnessScorer

client = JudgmentClient()
example = Example(
    input="...",
    actual_output="...",
    tools_called=["GoogleSearch", "Perplexity"],
    expected_output=["DBQuery", "GoogleSearch"],
)
# supply your own threshold
scorer = ToolCorrectnessScorer(threshold=0.8)

results = client.run_evaluation(
    examples=[example],
    scorers=[scorer],
    model="gpt-4o",
)
print(results)
```
