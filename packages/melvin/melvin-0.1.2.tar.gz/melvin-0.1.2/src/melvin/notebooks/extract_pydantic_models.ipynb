{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Array': <tecton_gen_ai.testing.examples.copilot._FuncOrClass>,\n",
       " 'Field': <tecton_gen_ai.testing.examples.copilot._FuncOrClass>,\n",
       " 'Map': <tecton_gen_ai.testing.examples.copilot._FuncOrClass>,\n",
       " 'SdkDataType': <tecton_gen_ai.testing.examples.copilot._FuncOrClass>,\n",
       " 'StrictFrozenModel': <tecton_gen_ai.testing.examples.copilot._FuncOrClass>,\n",
       " 'Struct': <tecton_gen_ai.testing.examples.copilot._FuncOrClass>,\n",
       " 'TectonValidationError': <tecton_gen_ai.testing.examples.copilot._FuncOrClass>}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import inspect\nfrom typing import Union, get_type_hints\nfrom src.tecton_gen_ai.testing.examples.copilot import _FuncOrClass\nimport enum\n\npool = {}\n\ndef get_full_name(obj):\n    return obj.__module__ + \".\" + obj.__name__\n\ndef is_tecton_type(tp):\n    try:\n        tp_str = tp.__module__\n        return tp_str.startswith(\"tecton.\") or tp_str==\"tecton\"\n    except Exception:\n        return False\n    \ndef is_abdstract(cls):\n    return inspect.isabstract(cls)\n\ndef find_tecton_parents(cls):\n    for parent in cls.__bases__:\n        if is_tecton_type(parent):\n            yield parent\n        \n\ndef find_tecton_annotations(cls):\n    try:\n        if inspect.isclass(cls):\n            annotations = get_type_hints(cls.__init__)\n        elif inspect.isfunction(cls):\n            annotations = get_type_hints(cls)\n        else:\n            raise ValueError(\"cls must be a class or a function\")\n        for name, param in annotations.items():\n            if is_tecton_type(param):\n                yield param\n            # else check if param is union, check if any of the union is a tecton type\n            elif hasattr(param, \"__origin__\") and param.__origin__ == Union:\n                for arg in param.__args__:\n                    if is_tecton_type(arg):\n                        yield arg\n    except Exception:\n        return None\n\ndef find_tecton_mentions(obj):\n    # check obj.obj is a class type use inspect\n    if inspect.isclass(obj):\n        if not issubclass(obj, enum.Enum):\n            yield from find_tecton_annotations(obj)\n    elif inspect.isfunction(obj):\n        yield from find_tecton_annotations(obj)\n\ndef find_tecton_dependencies(scope):\n    res = {}\n    for key, obj in scope.items():\n        res[key]= list(x.__name__ for x in find_tecton_mentions(obj.obj) if x.__name__ in scope)\n    return res\n\ndef build_api_graph():\n    objects = list(_FuncOrClass.from_expressions([\"tecton\"]))\n    scope = {x.name: x for x in objects}\n    deps = find_tecton_dependencies(scope)\n    res = {}\n    for key, value in scope.items():\n        res[key] = {\"declaration\": value.callable_declaration, \"deps\": deps[key]}\n    return res\n\ndef build_code(name, graph, code):\n    if name in code:\n        return\n    for dep in graph[name][\"deps\"]:\n        build_code(dep, graph, code)\n    code[name] = graph[name][\"declaration\"]\n\n\ngraph = build_api_graph()\n\ncode = {}\nbuild_code(\"BatchFeatureView\", graph, code)\nprint(\"\\n\\n\".join(code.values()))"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "import json\n\nall_class_names = sorted(set([x.__name__ for x in find_tecton_mentions(objects)]))\nall_class_names = json.dumps(all_class_names)\nall_class_names"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "from src.tecton_gen_ai.testing import set_dev_mode\nfrom src.tecton_gen_ai.api import Agent\nfrom pydantic import BaseModel, Field\n\nclass Output(BaseModel):\n    code: str = Field(..., description=\"The generated python code\")\n\nset_dev_mode()\n\nagent = Agent(\n    name = \"parser\",\n    prompt = \"\"\"\n    You are given a declaration of a class or a function, convert it to a pydantic model representation\n    You only need to output the pydantic model code, no import needed\n    You need to make sure each field is defined using pydantic Field, but you should use `PField` instead\n\n    parameters docstrings should not be included in the final class docstring\n\n    For example\n\n    ```python\n    class SomeView:\n        '''This is a class docstring for SomeView'''\n\n        def __init__(self, a: int, b: Entity):\n            '''\n            more explanations 1\n\n            :param a: parameter a\n            :param b: parameter b\n\n            more explanations 2\n            '''\n            pass\n    ```\n\n    should be translated to\n\n    ```python\n    class SomeView(BaseModel):\n        '''This is a class docstring for SomeView\n        \n        more explanations 1\n\n        more explanations 2\n        '''\n\n        a: int = PField(..., description=\"parameter a\")\n        b: Entity = PField(..., description=\"parameter b\")\n    ```\n\n    The output should just be the python code without backticks\n\n    \"\"\",\n    llm = {\"model\": \"openai/gpt-4o-2024-11-20\", \"temperature\": 0.0},\n    output_schema=Output,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:28<00:00,  5.62s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "code = []\n",
    "\n",
    "for obj in tqdm.tqdm(objects[:5]):\n",
    "    if inspect.isclass(obj.obj):\n",
    "        res = agent.invoke(obj.declaration)\n",
    "        code.append(res[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Aggregate(BaseModel):\n",
      "    '''The `Aggregate` class describes an aggregation feature that is applied to a Batch or Stream Feature View via `features` param.\n",
      "\n",
      "    ```python\n",
      "    from tecton import Aggregate, batch_feature_view, TimeWindow\n",
      "    from tecton.types import Int64\n",
      "    from datetime import timedelta\n",
      "\n",
      "    @batch_feature_view(\n",
      "        # ...\n",
      "        features=[\n",
      "            Aggregate(\n",
      "                input_column=Field(\"my_column\", Int64),\n",
      "                function=\"mean\",\n",
      "                time_window=TimeWindow(window_size=timedelta(days=7)),\n",
      "            ),\n",
      "            Aggregate(\n",
      "                input_column=Field(\"another_column\", Int64),\n",
      "                function=\"mean\",\n",
      "                time_window=TimeWindow(window_size=timedelta(days=1)),\n",
      "                name=\"1d_average\",\n",
      "                description=\"my aggregate feature description\",\n",
      "                tags={\"tag\": \"value\"}\n",
      "            ),\n",
      "        ],\n",
      "    )\n",
      "    def my_fv(data_source):\n",
      "        pass\n",
      "    ```\n",
      "    '''\n",
      "\n",
      "    function: AggregationFunction = PField(..., description=\"One of the built-in aggregation functions, such as 'sum', 'count', `last(2)` etc.\")\n",
      "    time_window: Union[TimeWindow, TimeWindowSeries, LifetimeWindow] = PField(..., description=\"The window_size and optional offset over which to aggregate over.\")\n",
      "    name: Optional[str] = PField(None, description=\"The name of this feature. Defaults to an autogenerated name, e.g. transaction_count_7d_1d.\")\n",
      "    input_column: Field = PField(..., description=\"Describes name and type of the column that will be used in the aggregation.\")\n",
      "    description: Optional[str] = PField(None, description=\"A human-readable description of the feature\")\n",
      "    tags: Optional[Dict[str, str]] = PField(None, description=\"Tags associated with the feature (key-value pairs of user-defined metadata).\")\n",
      "\n",
      "class AggregationLeadingEdge(BaseModel):\n",
      "    \"\"\"Defines the leading edge timestamp for aggregation windows in stream feature views during online retrieval.\n",
      "\n",
      "    WALL_CLOCK_TIME: Stream aggregation windows are fetched relative to the wall clock time at the time of online retrieval.\n",
      "    LATEST_EVENT_TIME: Stream aggregation windows are fetched relative the latest materialized event timestamp for the stream feature view. This timestamp is also known as the stream high watermark.\n",
      "\n",
      "    Example:\n",
      "        For a stream that is 30-seconds delayed and a 10-minute aggregation window:\n",
      "        - WALL_CLOCK_TIME at 12:00:00 uses the window [11:50:00, 12:00:00]\n",
      "        - LATEST_EVENT_TIME at 12:00:00 uses the window [11:49:30, 11:59:30]\n",
      "\n",
      "    Refer to documentation for detailed implications of each option.\n",
      "    \"\"\"\n",
      "\n",
      "    UNSPECIFIED: str = PField(..., description=\"Unspecified aggregation mode.\")\n",
      "    WALL_CLOCK_TIME: str = PField(..., description=\"Aggregation mode based on wall clock time.\")\n",
      "    LATEST_EVENT_TIME: str = PField(..., description=\"Aggregation mode based on the latest event time.\")\n",
      "\n",
      "class Attribute(BaseModel):\n",
      "    \"\"\"The `Attribute` class describes an attribute feature that is applied to a Feature View or Feature Table via `features` param.\n",
      "\n",
      "    ```python\n",
      "    from tecton import Attribute, batch_feature_view\n",
      "    from tecton.types import String\n",
      "\n",
      "    @batch_feature_view(\n",
      "        # ...\n",
      "        features=[\n",
      "            Attribute(\n",
      "                name=\"my_column\",\n",
      "                dtype=String\n",
      "            )\n",
      "            Attribute(\n",
      "                name=\"my_other_column\",\n",
      "                dtype=String,\n",
      "                description=\"my attribute feature description\",\n",
      "                tags={\"tag\": \"value\"}\n",
      "            )\n",
      "        ],\n",
      "    )\n",
      "    def my_fv(data_source):\n",
      "        pass\n",
      "    ```\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    name: str = PField(..., description=\"Name of a column from the transformation output.\")\n",
      "    dtype: SdkDataType = PField(..., description=\"Datatype of a column from the transformation output\")\n",
      "    description: Optional[str] = PField(None, description=\"A human-readable description of the feature\")\n",
      "    tags: Optional[Dict[str, str]] = PField(None, description=\"Tags associated with the feature (key-value pairs of user-defined metadata).\")\n",
      "\n",
      "class AutoscalingConfig(BaseModel):\n",
      "    \"\"\"Configuration for autoscaling of server groups.\"\"\"\n",
      "\n",
      "    min_nodes: Optional[int] = PField(None, description=\"The minimum number of nodes to scale down to.\")\n",
      "    max_nodes: Optional[int] = PField(None, description=\"The maximum number of nodes to scale up to.\")\n",
      "\n",
      "class BatchFeatureView(BaseModel):\n",
      "    \"\"\"A Tecton Batch Feature View, used for materializing features on a batch schedule from a BatchSource.\n",
      "\n",
      "    The BatchFeatureView should not be instantiated directly, the `@batch_feature_view`\n",
      "    decorator is recommended instead.\n",
      "\n",
      "    Attributes:\n",
      "        entities: The Entities for this Feature View.\n",
      "        info: A dataclass containing basic info about this Tecton Object.\n",
      "        sources: The Data Source inputs for this Feature View.\n",
      "        transformations: The Transformations used by this Feature View.\n",
      "    \"\"\"\n",
      "\n",
      "    name: str = PField(...)\n",
      "    description: Optional[str] = PField(None)\n",
      "    owner: Optional[str] = PField(None)\n",
      "    tags: Optional[Dict[str, str]] = PField(None)\n",
      "    prevent_destroy: bool = PField(False)\n",
      "    feature_view_function: Callable = PField(...)\n",
      "    sources: Sequence[Union[framework_data_source.BatchSource, FilteredSource]] = PField(...)\n",
      "    entities: Sequence[framework_entity.Entity] = PField(...)\n",
      "    mode: str = PField(...)\n",
      "    timestamp_field: str = PField(...)\n",
      "    features: Sequence[feature.Feature] = PField(...)\n",
      "    aggregation_interval: Optional[datetime.timedelta] = PField(None)\n",
      "    aggregation_secondary_key: Optional[str] = PField(None)\n",
      "    online: bool = PField(False)\n",
      "    offline: bool = PField(False)\n",
      "    ttl: Optional[datetime.timedelta] = PField(None)\n",
      "    feature_start_time: Optional[datetime.datetime] = PField(None)\n",
      "    lifetime_start_time: Optional[datetime.datetime] = PField(None)\n",
      "    manual_trigger_backfill_end_time: Optional[datetime.datetime] = PField(None)\n",
      "    batch_trigger: BatchTriggerType = PField(BatchTriggerType.SCHEDULED)\n",
      "    batch_schedule: Optional[datetime.timedelta] = PField(None)\n",
      "    online_serving_index: Optional[Sequence[str]] = PField(None)\n",
      "    batch_compute: Optional[configs.ComputeConfigTypes] = PField(None)\n",
      "    offline_store: Optional[Union[configs.OfflineStoreConfig, configs.ParquetConfig, configs.DeltaConfig]] = PField(None)\n",
      "    online_store: Optional[configs.OnlineStoreTypes] = PField(None)\n",
      "    monitor_freshness: bool = PField(False)\n",
      "    data_quality_enabled: Optional[bool] = PField(None)\n",
      "    skip_default_expectations: Optional[bool] = PField(None)\n",
      "    expected_feature_freshness: Optional[datetime.timedelta] = PField(None)\n",
      "    alert_email: Optional[str] = PField(None)\n",
      "    max_backfill_interval: Optional[datetime.timedelta] = PField(None)\n",
      "    incremental_backfills: bool = PField(False)\n",
      "    run_transformation_validation: Optional[bool] = PField(None)\n",
      "    options: Optional[Dict[str, str]] = PField(None)\n",
      "    tecton_materialization_runtime: Optional[str] = PField(None)\n",
      "    cache_config: Optional[configs.CacheConfig] = PField(None)\n",
      "    compaction_enabled: bool = PField(False)\n",
      "    environment: Optional[str] = PField(None)\n",
      "    context_parameter_name: Optional[str] = PField(None)\n",
      "    secrets: Optional[Dict[str, Union[Secret, str]]] = PField(None)\n",
      "    resource_providers: Optional[Dict[str, resource_provider.ResourceProvider]] = PField(None)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n\".join(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}