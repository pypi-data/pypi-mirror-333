# -*- coding: utf-8 -*-
"""AzureOpenAI React_agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oxiYZZmqktlxVook6t3aVo0QgRCD2qBU
"""

#!pip install -U llama-index -q
#!pip install -U ragaai-catalyst==2.1.5.b27 -q
#!pip install llama-index-llms-azure-openai -q



import nest_asyncio
nest_asyncio.apply()

import os
from llama_index.llms.azure_openai import AzureOpenAI
from dotenv import load_dotenv

load_dotenv() 

endpoint = os.environ["AZURE_OPENAI_ENDPOINT"]
deployment = os.environ["AZURE_DEPLOYMENT"]
subscription_key = os.environ["AZURE_SUBSCRIPTION_KEY"]
model = "gpt-4o-mini"

llm = AzureOpenAI(
    azure_endpoint=endpoint,
    model = model,
    api_key=subscription_key,
    api_version="2024-05-01-preview",
    engine=deployment
    
)

base_url=os.environ["RAGAAI_CATALYST_STAGING_BASE_URL"]
secret_key=os.environ["RAGAAI_CATALYST_STAGING_SECRET_KEY"]
access_key=os.environ["RAGAAI_CATALYST_STAGING_ACCESS_KEY"]

"""**Initialize Tracer**"""

from ragaai_catalyst.tracers import Tracer
from ragaai_catalyst import RagaAICatalyst, init_tracing
from ragaai_catalyst import trace_llm,trace_tool, trace_agent

catalyst = RagaAICatalyst(
    access_key=access_key,
    secret_key=secret_key,
    base_url=base_url
)

# Initialize tracer
tracer = Tracer(
    project_name="LLAMAINDEX-AzureOpenAI",
    dataset_name="ReactAgent_tracing_fixed",
    tracer_type="Agentic",
)
tracer.set_model_cost({"model_name":deployment,"input_cost_per_million_token":6,"output_cost_per_million_token":2.40})
init_tracing(catalyst=catalyst, tracer=tracer)

"""**Workflow Events**"""

from llama_index.core.llms import ChatMessage
from llama_index.core.tools import ToolSelection, ToolOutput
from llama_index.core.workflow import Event


class PrepEvent(Event):
    pass


class InputEvent(Event):
    input: list[ChatMessage]


class ToolCallEvent(Event):
    tool_calls: list[ToolSelection]


class FunctionOutputEvent(Event):
    output: ToolOutput

"""**The Workflow**"""

from typing import Any, List

from llama_index.core.agent.react import ReActChatFormatter, ReActOutputParser
from llama_index.core.agent.react.types import (
    ActionReasoningStep,
    ObservationReasoningStep,
)
from llama_index.core.llms.llm import LLM
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core.tools.types import BaseTool
from llama_index.core.workflow import (
    Context,
    Workflow,
    StartEvent,
    StopEvent,
    step,
)
from llama_index.llms.openai import OpenAI


class ReActAgent(Workflow):
    def __init__(
        self,
        *args: Any,
        llm: LLM | None = None,
        tools: list[BaseTool] | None = None,
        extra_context: str | None = None,
        **kwargs: Any,
    ) -> None:
        super().__init__(*args, **kwargs)
        self.tools = tools or []

        self.llm = llm or OpenAI()

        self.memory = ChatMemoryBuffer.from_defaults(llm=llm)
        self.formatter = ReActChatFormatter.from_defaults(
            context=extra_context or ""
        )
        self.output_parser = ReActOutputParser()
        self.sources = []

    @step
    @trace_agent("new user message")
    async def new_user_msg(self, ctx: Context, ev: StartEvent) -> PrepEvent:
        # clear sources
        self.sources = []

        # get user input
        user_input = ev.input
        user_msg = ChatMessage(role="user", content=user_input)
        self.memory.put(user_msg)

        # clear current reasoning
        await ctx.set("current_reasoning", [])

        return PrepEvent()

    @step
    async def prepare_chat_history(
        self, ctx: Context, ev: PrepEvent
    ) -> InputEvent:
        # get chat history
        chat_history = self.memory.get()
        current_reasoning = await ctx.get("current_reasoning", default=[])
        llm_input = self.formatter.format(
            self.tools, chat_history, current_reasoning=current_reasoning
        )
        return InputEvent(input=llm_input)

    @step
    @trace_tool("handle llm input")
    async def handle_llm_input(
        self, ctx: Context, ev: InputEvent
    ) -> ToolCallEvent | StopEvent:
        chat_history = ev.input

        response = await self.llm.achat(chat_history)

        try:
            reasoning_step = self.output_parser.parse(response.message.content)
            (await ctx.get("current_reasoning", default=[])).append(
                reasoning_step
            )
            if reasoning_step.is_done:
                self.memory.put(
                    ChatMessage(
                        role="assistant", content=reasoning_step.response
                    )
                )
                return StopEvent(
                    result={
                        "response": reasoning_step.response,
                        "sources": [*self.sources],
                        "reasoning": await ctx.get(
                            "current_reasoning", default=[]
                        ),
                    }
                )
            elif isinstance(reasoning_step, ActionReasoningStep):
                tool_name = reasoning_step.action
                tool_args = reasoning_step.action_input
                return ToolCallEvent(
                    tool_calls=[
                        ToolSelection(
                            tool_id="fake",
                            tool_name=tool_name,
                            tool_kwargs=tool_args,
                        )
                    ]
                )
        except Exception as e:
            (await ctx.get("current_reasoning", default=[])).append(
                ObservationReasoningStep(
                    observation=f"There was an error in parsing my reasoning: {e}"
                )
            )

        # if no tool calls or final response, iterate again
        return PrepEvent()

    @step
    @trace_tool("Reasoning steps")
    async def handle_tool_calls(
        self, ctx: Context, ev: ToolCallEvent
    ) -> PrepEvent:
        tool_calls = ev.tool_calls
        tools_by_name = {tool.metadata.get_name(): tool for tool in self.tools}

        # call tools -- safely!
        for tool_call in tool_calls:
            tool = tools_by_name.get(tool_call.tool_name)
            if not tool:
                (await ctx.get("current_reasoning", default=[])).append(
                    ObservationReasoningStep(
                        observation=f"Tool {tool_call.tool_name} does not exist"
                    )
                )
                continue

            try:
                tool_output = tool(**tool_call.tool_kwargs)
                self.sources.append(tool_output)
                (await ctx.get("current_reasoning", default=[])).append(
                    ObservationReasoningStep(observation=tool_output.content)
                )
            except Exception as e:
                (await ctx.get("current_reasoning", default=[])).append(
                    ObservationReasoningStep(
                        observation=f"Error calling tool {tool.metadata.get_name()}: {e}"
                    )
                )

        # prep the next iteraiton
        return PrepEvent()

"""**Run the Workflow**"""

from llama_index.core.tools import FunctionTool
from llama_index.llms.azure_openai import AzureOpenAI

def add(x: int, y: int) -> int:
    """Useful function to add two numbers."""
    return x + y


def multiply(x: int, y: int) -> int:
    """Useful function to multiply two numbers."""
    return x * y


tools = [
    FunctionTool.from_defaults(add),
    FunctionTool.from_defaults(multiply),
]

agent = ReActAgent(
    llm = AzureOpenAI(
    azure_endpoint=endpoint,
    api_key=subscription_key,
    api_version="2024-05-01-preview",
    engine=deployment,model="gpt-4o-mini"), tools=tools, timeout=120, verbose=True
)

# Add this async function to wrap the agent calls
async def main():
    with tracer:
        ret = await agent.run(input="Hello!")
        print(ret["response"])
        ret = await agent.run(input="What is (2123 + 2321) * 312?")
        print(ret["response"])

# Add this to run the async function
if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

